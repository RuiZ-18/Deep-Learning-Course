{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 实验前导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tud\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mnist_data = datasets.MNIST(\"./mnist_data\",train=True,download=True,\n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.13066062,),std=(0.30810776,))\n",
    "                                 ]))                             \n",
    "batch_size = 32\n",
    "dataloader = tud.DataLoader(mnist_data,batch_size = batch_size,shuffle=True)          # 将dataset转换为iterator\n",
    "'''\n",
    "mnist_data = datasets.MNIST(\"./mnist_data\",train=True,download=True,\n",
    "                                 transform = transforms.Compose([\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.5,),std=(0.5,))\n",
    "                                 ]))\n",
    "batch_size = 32\n",
    "dataloader = tud.DataLoader(mnist_data,batch_size = batch_size,shuffle=True)          # 将dataset转换为iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 配置网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28 * 28\n",
    "hidden_size = 256\n",
    "# 判别器\n",
    "# write your code here\n",
    "# 写一个判别器\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "latent_size = 64\n",
    "# 生成器\n",
    "# write your code here\n",
    "# G = nn.Sequential(....)\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "\n",
    "# define your loss function\n",
    "# write your code here\n",
    "loss_fn = nn.BCELoss()\n",
    "# define your optimizer\n",
    "# write your code here(需要分别定义生成器和判别器的优化器)\n",
    "d_optimizer = torch.optim.Adam(D.parameters(),lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(),lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/30], Step [0/1875], d_loss:1.3890, g_loss:0.6906, D(x):0.50, D(G(z)):0.50, G(z):0.50\n",
      "Epoch [0/30], Step [200/1875], d_loss:0.5552, g_loss:1.6737, D(x):0.91, D(G(z)):0.37, G(z):0.19\n",
      "Epoch [0/30], Step [400/1875], d_loss:0.0331, g_loss:6.2632, D(x):0.99, D(G(z)):0.02, G(z):0.00\n",
      "Epoch [0/30], Step [600/1875], d_loss:0.2042, g_loss:2.8373, D(x):0.99, D(G(z)):0.17, G(z):0.07\n",
      "Epoch [0/30], Step [800/1875], d_loss:0.6560, g_loss:2.2893, D(x):0.87, D(G(z)):0.36, G(z):0.11\n",
      "Epoch [0/30], Step [1000/1875], d_loss:0.1131, g_loss:5.0538, D(x):0.97, D(G(z)):0.07, G(z):0.02\n",
      "Epoch [0/30], Step [1200/1875], d_loss:0.7589, g_loss:1.7265, D(x):0.90, D(G(z)):0.44, G(z):0.19\n",
      "Epoch [0/30], Step [1400/1875], d_loss:1.4943, g_loss:1.8590, D(x):0.54, D(G(z)):0.28, G(z):0.20\n",
      "Epoch [0/30], Step [1600/1875], d_loss:0.4531, g_loss:2.2852, D(x):0.81, D(G(z)):0.11, G(z):0.12\n",
      "Epoch [0/30], Step [1800/1875], d_loss:0.9846, g_loss:2.1150, D(x):0.64, D(G(z)):0.31, G(z):0.15\n",
      "Epoch [1/30], Step [0/1875], d_loss:1.2931, g_loss:1.7408, D(x):0.57, D(G(z)):0.23, G(z):0.23\n",
      "Epoch [1/30], Step [200/1875], d_loss:0.2495, g_loss:3.3349, D(x):0.86, D(G(z)):0.07, G(z):0.06\n",
      "Epoch [1/30], Step [400/1875], d_loss:0.2206, g_loss:3.1173, D(x):0.95, D(G(z)):0.14, G(z):0.08\n",
      "Epoch [1/30], Step [600/1875], d_loss:0.5660, g_loss:2.2830, D(x):0.76, D(G(z)):0.20, G(z):0.11\n",
      "Epoch [1/30], Step [800/1875], d_loss:0.6595, g_loss:1.5885, D(x):0.84, D(G(z)):0.36, G(z):0.21\n",
      "Epoch [1/30], Step [1000/1875], d_loss:0.8395, g_loss:1.3382, D(x):0.78, D(G(z)):0.37, G(z):0.30\n",
      "Epoch [1/30], Step [1200/1875], d_loss:1.8138, g_loss:0.8860, D(x):0.46, D(G(z)):0.54, G(z):0.43\n",
      "Epoch [1/30], Step [1400/1875], d_loss:2.2229, g_loss:0.5840, D(x):0.52, D(G(z)):0.62, G(z):0.59\n",
      "Epoch [1/30], Step [1600/1875], d_loss:1.4653, g_loss:0.7049, D(x):0.51, D(G(z)):0.44, G(z):0.52\n",
      "Epoch [1/30], Step [1800/1875], d_loss:1.8003, g_loss:0.6209, D(x):0.60, D(G(z)):0.66, G(z):0.56\n",
      "Epoch [2/30], Step [0/1875], d_loss:1.2100, g_loss:1.0501, D(x):0.55, D(G(z)):0.39, G(z):0.37\n",
      "Epoch [2/30], Step [200/1875], d_loss:1.1312, g_loss:0.9955, D(x):0.58, D(G(z)):0.39, G(z):0.38\n",
      "Epoch [2/30], Step [400/1875], d_loss:0.8794, g_loss:1.3020, D(x):0.64, D(G(z)):0.32, G(z):0.28\n",
      "Epoch [2/30], Step [600/1875], d_loss:1.2089, g_loss:1.3147, D(x):0.54, D(G(z)):0.30, G(z):0.29\n",
      "Epoch [2/30], Step [800/1875], d_loss:0.3393, g_loss:3.5266, D(x):0.85, D(G(z)):0.10, G(z):0.03\n",
      "Epoch [2/30], Step [1000/1875], d_loss:0.5636, g_loss:2.6116, D(x):0.80, D(G(z)):0.13, G(z):0.09\n",
      "Epoch [2/30], Step [1200/1875], d_loss:0.7948, g_loss:2.1590, D(x):0.86, D(G(z)):0.36, G(z):0.17\n",
      "Epoch [2/30], Step [1400/1875], d_loss:3.5702, g_loss:1.2839, D(x):0.37, D(G(z)):0.56, G(z):0.46\n",
      "Epoch [2/30], Step [1600/1875], d_loss:1.3438, g_loss:1.0992, D(x):0.53, D(G(z)):0.33, G(z):0.36\n",
      "Epoch [2/30], Step [1800/1875], d_loss:0.9920, g_loss:1.7759, D(x):0.64, D(G(z)):0.30, G(z):0.23\n",
      "Epoch [3/30], Step [0/1875], d_loss:0.4373, g_loss:1.9536, D(x):0.86, D(G(z)):0.22, G(z):0.16\n",
      "Epoch [3/30], Step [200/1875], d_loss:0.3307, g_loss:2.2189, D(x):0.88, D(G(z)):0.16, G(z):0.12\n",
      "Epoch [3/30], Step [400/1875], d_loss:0.2368, g_loss:2.4264, D(x):0.96, D(G(z)):0.17, G(z):0.11\n",
      "Epoch [3/30], Step [600/1875], d_loss:0.7720, g_loss:2.5367, D(x):0.74, D(G(z)):0.16, G(z):0.12\n",
      "Epoch [3/30], Step [800/1875], d_loss:0.5987, g_loss:1.5794, D(x):0.83, D(G(z)):0.27, G(z):0.23\n",
      "Epoch [3/30], Step [1000/1875], d_loss:1.0313, g_loss:2.4890, D(x):0.62, D(G(z)):0.15, G(z):0.14\n",
      "Epoch [3/30], Step [1200/1875], d_loss:1.2053, g_loss:1.8707, D(x):0.74, D(G(z)):0.42, G(z):0.21\n",
      "Epoch [3/30], Step [1400/1875], d_loss:0.6318, g_loss:2.2838, D(x):0.78, D(G(z)):0.17, G(z):0.14\n",
      "Epoch [3/30], Step [1600/1875], d_loss:0.3210, g_loss:2.9224, D(x):0.86, D(G(z)):0.08, G(z):0.07\n",
      "Epoch [3/30], Step [1800/1875], d_loss:1.0024, g_loss:2.1615, D(x):0.69, D(G(z)):0.16, G(z):0.14\n",
      "Epoch [4/30], Step [0/1875], d_loss:1.2306, g_loss:2.3360, D(x):0.81, D(G(z)):0.39, G(z):0.17\n",
      "Epoch [4/30], Step [200/1875], d_loss:0.5900, g_loss:3.0629, D(x):0.82, D(G(z)):0.17, G(z):0.11\n",
      "Epoch [4/30], Step [400/1875], d_loss:0.6523, g_loss:3.0643, D(x):0.97, D(G(z)):0.35, G(z):0.12\n",
      "Epoch [4/30], Step [600/1875], d_loss:0.3791, g_loss:2.7302, D(x):0.86, D(G(z)):0.12, G(z):0.11\n",
      "Epoch [4/30], Step [800/1875], d_loss:0.2898, g_loss:2.6669, D(x):0.94, D(G(z)):0.18, G(z):0.09\n",
      "Epoch [4/30], Step [1000/1875], d_loss:0.2056, g_loss:4.5248, D(x):0.98, D(G(z)):0.15, G(z):0.02\n",
      "Epoch [4/30], Step [1200/1875], d_loss:0.4473, g_loss:4.0221, D(x):0.85, D(G(z)):0.04, G(z):0.03\n",
      "Epoch [4/30], Step [1400/1875], d_loss:0.3672, g_loss:2.7054, D(x):0.96, D(G(z)):0.23, G(z):0.12\n",
      "Epoch [4/30], Step [1600/1875], d_loss:0.6121, g_loss:3.2347, D(x):0.86, D(G(z)):0.11, G(z):0.06\n",
      "Epoch [4/30], Step [1800/1875], d_loss:0.5364, g_loss:2.4449, D(x):0.82, D(G(z)):0.18, G(z):0.12\n",
      "Epoch [5/30], Step [0/1875], d_loss:0.7980, g_loss:1.4297, D(x):0.89, D(G(z)):0.41, G(z):0.36\n",
      "Epoch [5/30], Step [200/1875], d_loss:0.6386, g_loss:2.7365, D(x):0.80, D(G(z)):0.17, G(z):0.14\n",
      "Epoch [5/30], Step [400/1875], d_loss:0.9323, g_loss:1.9490, D(x):0.73, D(G(z)):0.27, G(z):0.21\n",
      "Epoch [5/30], Step [600/1875], d_loss:0.8908, g_loss:1.7761, D(x):0.73, D(G(z)):0.31, G(z):0.27\n",
      "Epoch [5/30], Step [800/1875], d_loss:0.7133, g_loss:1.8891, D(x):0.80, D(G(z)):0.22, G(z):0.22\n",
      "Epoch [5/30], Step [1000/1875], d_loss:0.6264, g_loss:2.6332, D(x):0.78, D(G(z)):0.16, G(z):0.11\n",
      "Epoch [5/30], Step [1200/1875], d_loss:0.2821, g_loss:3.3354, D(x):0.91, D(G(z)):0.12, G(z):0.07\n",
      "Epoch [5/30], Step [1400/1875], d_loss:1.0901, g_loss:1.8867, D(x):0.72, D(G(z)):0.39, G(z):0.22\n",
      "Epoch [5/30], Step [1600/1875], d_loss:0.7106, g_loss:1.8026, D(x):0.77, D(G(z)):0.17, G(z):0.21\n",
      "Epoch [5/30], Step [1800/1875], d_loss:0.5217, g_loss:2.4631, D(x):0.87, D(G(z)):0.25, G(z):0.16\n",
      "Epoch [6/30], Step [0/1875], d_loss:0.4983, g_loss:3.1804, D(x):0.80, D(G(z)):0.08, G(z):0.06\n",
      "Epoch [6/30], Step [200/1875], d_loss:0.6388, g_loss:2.3715, D(x):0.72, D(G(z)):0.10, G(z):0.16\n",
      "Epoch [6/30], Step [400/1875], d_loss:1.4410, g_loss:1.1049, D(x):0.55, D(G(z)):0.30, G(z):0.42\n",
      "Epoch [6/30], Step [600/1875], d_loss:0.5757, g_loss:2.3134, D(x):0.82, D(G(z)):0.22, G(z):0.16\n",
      "Epoch [6/30], Step [800/1875], d_loss:1.0796, g_loss:1.8198, D(x):0.65, D(G(z)):0.21, G(z):0.21\n",
      "Epoch [6/30], Step [1000/1875], d_loss:0.5715, g_loss:2.7146, D(x):0.79, D(G(z)):0.19, G(z):0.14\n",
      "Epoch [6/30], Step [1200/1875], d_loss:0.5927, g_loss:2.3470, D(x):0.79, D(G(z)):0.21, G(z):0.15\n",
      "Epoch [6/30], Step [1400/1875], d_loss:0.7314, g_loss:2.4414, D(x):0.74, D(G(z)):0.19, G(z):0.14\n",
      "Epoch [6/30], Step [1600/1875], d_loss:0.5049, g_loss:2.2786, D(x):0.83, D(G(z)):0.21, G(z):0.18\n",
      "Epoch [6/30], Step [1800/1875], d_loss:0.4973, g_loss:1.6014, D(x):0.89, D(G(z)):0.27, G(z):0.24\n",
      "Epoch [7/30], Step [0/1875], d_loss:1.0474, g_loss:1.5761, D(x):0.62, D(G(z)):0.21, G(z):0.30\n",
      "Epoch [7/30], Step [200/1875], d_loss:0.4378, g_loss:3.2765, D(x):0.84, D(G(z)):0.13, G(z):0.06\n",
      "Epoch [7/30], Step [400/1875], d_loss:0.4645, g_loss:2.1982, D(x):0.89, D(G(z)):0.25, G(z):0.16\n",
      "Epoch [7/30], Step [600/1875], d_loss:0.3303, g_loss:2.2078, D(x):0.87, D(G(z)):0.15, G(z):0.14\n",
      "Epoch [7/30], Step [800/1875], d_loss:0.4131, g_loss:3.2527, D(x):0.84, D(G(z)):0.12, G(z):0.06\n",
      "Epoch [7/30], Step [1000/1875], d_loss:0.5961, g_loss:3.0297, D(x):0.82, D(G(z)):0.13, G(z):0.11\n",
      "Epoch [7/30], Step [1200/1875], d_loss:0.3612, g_loss:3.3234, D(x):0.85, D(G(z)):0.13, G(z):0.08\n",
      "Epoch [7/30], Step [1400/1875], d_loss:0.8545, g_loss:2.7087, D(x):0.75, D(G(z)):0.17, G(z):0.15\n",
      "Epoch [7/30], Step [1600/1875], d_loss:0.9827, g_loss:1.4667, D(x):0.89, D(G(z)):0.44, G(z):0.32\n",
      "Epoch [7/30], Step [1800/1875], d_loss:0.5588, g_loss:2.1189, D(x):0.91, D(G(z)):0.30, G(z):0.19\n",
      "Epoch [8/30], Step [0/1875], d_loss:0.2937, g_loss:3.0935, D(x):0.93, D(G(z)):0.17, G(z):0.08\n",
      "Epoch [8/30], Step [200/1875], d_loss:0.3787, g_loss:2.4979, D(x):0.90, D(G(z)):0.19, G(z):0.15\n",
      "Epoch [8/30], Step [400/1875], d_loss:1.0432, g_loss:1.9033, D(x):0.73, D(G(z)):0.26, G(z):0.25\n",
      "Epoch [8/30], Step [600/1875], d_loss:0.8955, g_loss:2.5146, D(x):0.69, D(G(z)):0.16, G(z):0.19\n",
      "Epoch [8/30], Step [800/1875], d_loss:0.5491, g_loss:3.1642, D(x):0.76, D(G(z)):0.11, G(z):0.09\n",
      "Epoch [8/30], Step [1000/1875], d_loss:0.5023, g_loss:2.3715, D(x):0.80, D(G(z)):0.20, G(z):0.14\n",
      "Epoch [8/30], Step [1200/1875], d_loss:0.6055, g_loss:1.9637, D(x):0.79, D(G(z)):0.21, G(z):0.20\n",
      "Epoch [8/30], Step [1400/1875], d_loss:0.5257, g_loss:1.8749, D(x):0.89, D(G(z)):0.28, G(z):0.22\n",
      "Epoch [8/30], Step [1600/1875], d_loss:0.4121, g_loss:2.9890, D(x):0.85, D(G(z)):0.14, G(z):0.08\n",
      "Epoch [8/30], Step [1800/1875], d_loss:0.4693, g_loss:2.6888, D(x):0.82, D(G(z)):0.14, G(z):0.12\n",
      "Epoch [9/30], Step [0/1875], d_loss:0.7179, g_loss:2.1452, D(x):0.79, D(G(z)):0.25, G(z):0.18\n",
      "Epoch [9/30], Step [200/1875], d_loss:0.8665, g_loss:2.3232, D(x):0.71, D(G(z)):0.22, G(z):0.15\n",
      "Epoch [9/30], Step [400/1875], d_loss:0.5815, g_loss:2.1901, D(x):0.79, D(G(z)):0.15, G(z):0.16\n",
      "Epoch [9/30], Step [600/1875], d_loss:0.9008, g_loss:2.8076, D(x):0.73, D(G(z)):0.19, G(z):0.16\n",
      "Epoch [9/30], Step [800/1875], d_loss:0.4512, g_loss:2.7544, D(x):0.84, D(G(z)):0.15, G(z):0.11\n",
      "Epoch [9/30], Step [1000/1875], d_loss:0.8133, g_loss:2.2542, D(x):0.66, D(G(z)):0.16, G(z):0.18\n",
      "Epoch [9/30], Step [1200/1875], d_loss:0.6533, g_loss:2.5435, D(x):0.77, D(G(z)):0.13, G(z):0.14\n",
      "Epoch [9/30], Step [1400/1875], d_loss:0.7462, g_loss:2.1627, D(x):0.80, D(G(z)):0.30, G(z):0.20\n",
      "Epoch [9/30], Step [1600/1875], d_loss:1.0523, g_loss:2.0582, D(x):0.74, D(G(z)):0.32, G(z):0.26\n",
      "Epoch [9/30], Step [1800/1875], d_loss:0.6628, g_loss:1.8814, D(x):0.88, D(G(z)):0.32, G(z):0.23\n",
      "Epoch [10/30], Step [0/1875], d_loss:0.8345, g_loss:1.5387, D(x):0.82, D(G(z)):0.31, G(z):0.31\n",
      "Epoch [10/30], Step [200/1875], d_loss:0.8592, g_loss:1.5999, D(x):0.74, D(G(z)):0.29, G(z):0.29\n",
      "Epoch [10/30], Step [400/1875], d_loss:0.2500, g_loss:2.9093, D(x):0.90, D(G(z)):0.11, G(z):0.09\n",
      "Epoch [10/30], Step [600/1875], d_loss:0.8288, g_loss:2.1523, D(x):0.75, D(G(z)):0.23, G(z):0.20\n",
      "Epoch [10/30], Step [800/1875], d_loss:0.7897, g_loss:1.3488, D(x):0.83, D(G(z)):0.35, G(z):0.34\n",
      "Epoch [10/30], Step [1000/1875], d_loss:0.6739, g_loss:1.9675, D(x):0.82, D(G(z)):0.27, G(z):0.25\n",
      "Epoch [10/30], Step [1200/1875], d_loss:0.4850, g_loss:2.3065, D(x):0.85, D(G(z)):0.20, G(z):0.17\n",
      "Epoch [10/30], Step [1400/1875], d_loss:0.6872, g_loss:2.7910, D(x):0.71, D(G(z)):0.12, G(z):0.10\n",
      "Epoch [10/30], Step [1600/1875], d_loss:0.5798, g_loss:2.5393, D(x):0.83, D(G(z)):0.20, G(z):0.14\n",
      "Epoch [10/30], Step [1800/1875], d_loss:1.0712, g_loss:2.0205, D(x):0.69, D(G(z)):0.27, G(z):0.22\n",
      "Epoch [11/30], Step [0/1875], d_loss:0.8179, g_loss:2.0253, D(x):0.82, D(G(z)):0.32, G(z):0.24\n",
      "Epoch [11/30], Step [200/1875], d_loss:0.7017, g_loss:2.1846, D(x):0.79, D(G(z)):0.26, G(z):0.17\n",
      "Epoch [11/30], Step [400/1875], d_loss:0.7551, g_loss:2.1948, D(x):0.75, D(G(z)):0.23, G(z):0.19\n",
      "Epoch [11/30], Step [600/1875], d_loss:0.7962, g_loss:2.3535, D(x):0.67, D(G(z)):0.15, G(z):0.15\n",
      "Epoch [11/30], Step [800/1875], d_loss:0.5410, g_loss:2.0179, D(x):0.87, D(G(z)):0.25, G(z):0.20\n",
      "Epoch [11/30], Step [1000/1875], d_loss:0.8601, g_loss:1.2913, D(x):0.79, D(G(z)):0.36, G(z):0.32\n",
      "Epoch [11/30], Step [1200/1875], d_loss:0.6753, g_loss:2.4628, D(x):0.76, D(G(z)):0.20, G(z):0.15\n",
      "Epoch [11/30], Step [1400/1875], d_loss:0.4159, g_loss:2.3465, D(x):0.88, D(G(z)):0.17, G(z):0.15\n",
      "Epoch [11/30], Step [1600/1875], d_loss:0.9862, g_loss:1.6574, D(x):0.67, D(G(z)):0.26, G(z):0.27\n",
      "Epoch [11/30], Step [1800/1875], d_loss:0.8093, g_loss:1.6618, D(x):0.74, D(G(z)):0.29, G(z):0.28\n",
      "Epoch [12/30], Step [0/1875], d_loss:1.1378, g_loss:1.4342, D(x):0.71, D(G(z)):0.38, G(z):0.32\n",
      "Epoch [12/30], Step [200/1875], d_loss:0.7528, g_loss:3.1810, D(x):0.71, D(G(z)):0.09, G(z):0.08\n",
      "Epoch [12/30], Step [400/1875], d_loss:0.7845, g_loss:1.9427, D(x):0.84, D(G(z)):0.36, G(z):0.24\n",
      "Epoch [12/30], Step [600/1875], d_loss:0.6121, g_loss:1.7326, D(x):0.83, D(G(z)):0.26, G(z):0.27\n",
      "Epoch [12/30], Step [800/1875], d_loss:0.7017, g_loss:2.3479, D(x):0.77, D(G(z)):0.19, G(z):0.15\n",
      "Epoch [12/30], Step [1000/1875], d_loss:0.4436, g_loss:2.5325, D(x):0.78, D(G(z)):0.12, G(z):0.13\n",
      "Epoch [12/30], Step [1200/1875], d_loss:0.7381, g_loss:1.5062, D(x):0.83, D(G(z)):0.30, G(z):0.31\n",
      "Epoch [12/30], Step [1400/1875], d_loss:1.0088, g_loss:1.7268, D(x):0.67, D(G(z)):0.29, G(z):0.30\n",
      "Epoch [12/30], Step [1600/1875], d_loss:0.6191, g_loss:2.5347, D(x):0.84, D(G(z)):0.23, G(z):0.18\n",
      "Epoch [12/30], Step [1800/1875], d_loss:0.8227, g_loss:1.7677, D(x):0.88, D(G(z)):0.37, G(z):0.29\n",
      "Epoch [13/30], Step [0/1875], d_loss:0.5332, g_loss:2.8400, D(x):0.82, D(G(z)):0.17, G(z):0.14\n",
      "Epoch [13/30], Step [200/1875], d_loss:0.4072, g_loss:2.2430, D(x):0.91, D(G(z)):0.20, G(z):0.18\n",
      "Epoch [13/30], Step [400/1875], d_loss:1.0913, g_loss:1.8945, D(x):0.61, D(G(z)):0.19, G(z):0.20\n",
      "Epoch [13/30], Step [600/1875], d_loss:0.7985, g_loss:2.1791, D(x):0.70, D(G(z)):0.21, G(z):0.17\n",
      "Epoch [13/30], Step [800/1875], d_loss:0.7113, g_loss:1.6299, D(x):0.74, D(G(z)):0.24, G(z):0.25\n",
      "Epoch [13/30], Step [1000/1875], d_loss:0.7848, g_loss:1.7645, D(x):0.69, D(G(z)):0.24, G(z):0.23\n",
      "Epoch [13/30], Step [1200/1875], d_loss:0.8858, g_loss:1.7440, D(x):0.75, D(G(z)):0.33, G(z):0.27\n",
      "Epoch [13/30], Step [1400/1875], d_loss:0.9187, g_loss:2.4287, D(x):0.66, D(G(z)):0.19, G(z):0.16\n",
      "Epoch [13/30], Step [1600/1875], d_loss:0.6356, g_loss:1.7972, D(x):0.81, D(G(z)):0.28, G(z):0.23\n",
      "Epoch [13/30], Step [1800/1875], d_loss:0.6396, g_loss:1.7238, D(x):0.77, D(G(z)):0.22, G(z):0.23\n",
      "Epoch [14/30], Step [0/1875], d_loss:1.0751, g_loss:1.4362, D(x):0.72, D(G(z)):0.32, G(z):0.31\n",
      "Epoch [14/30], Step [200/1875], d_loss:0.6963, g_loss:1.8602, D(x):0.76, D(G(z)):0.26, G(z):0.24\n",
      "Epoch [14/30], Step [400/1875], d_loss:0.8782, g_loss:2.0687, D(x):0.73, D(G(z)):0.26, G(z):0.22\n",
      "Epoch [14/30], Step [600/1875], d_loss:0.6911, g_loss:2.7056, D(x):0.69, D(G(z)):0.16, G(z):0.11\n",
      "Epoch [14/30], Step [800/1875], d_loss:0.6910, g_loss:1.9926, D(x):0.78, D(G(z)):0.22, G(z):0.20\n",
      "Epoch [14/30], Step [1000/1875], d_loss:0.8101, g_loss:2.0281, D(x):0.66, D(G(z)):0.20, G(z):0.20\n",
      "Epoch [14/30], Step [1200/1875], d_loss:1.1039, g_loss:1.7146, D(x):0.70, D(G(z)):0.38, G(z):0.31\n",
      "Epoch [14/30], Step [1400/1875], d_loss:0.7834, g_loss:1.5657, D(x):0.77, D(G(z)):0.30, G(z):0.27\n",
      "Epoch [14/30], Step [1600/1875], d_loss:0.9139, g_loss:1.9397, D(x):0.66, D(G(z)):0.24, G(z):0.20\n",
      "Epoch [14/30], Step [1800/1875], d_loss:0.5865, g_loss:1.9588, D(x):0.90, D(G(z)):0.29, G(z):0.23\n",
      "Epoch [15/30], Step [0/1875], d_loss:0.8221, g_loss:1.6809, D(x):0.77, D(G(z)):0.33, G(z):0.26\n",
      "Epoch [15/30], Step [200/1875], d_loss:0.8783, g_loss:1.5886, D(x):0.67, D(G(z)):0.25, G(z):0.27\n",
      "Epoch [15/30], Step [400/1875], d_loss:0.7921, g_loss:2.1087, D(x):0.70, D(G(z)):0.22, G(z):0.20\n",
      "Epoch [15/30], Step [600/1875], d_loss:0.7629, g_loss:1.7326, D(x):0.73, D(G(z)):0.22, G(z):0.25\n",
      "Epoch [15/30], Step [800/1875], d_loss:0.6205, g_loss:1.9358, D(x):0.75, D(G(z)):0.22, G(z):0.20\n",
      "Epoch [15/30], Step [1000/1875], d_loss:0.6726, g_loss:2.1969, D(x):0.73, D(G(z)):0.18, G(z):0.16\n",
      "Epoch [15/30], Step [1200/1875], d_loss:0.8204, g_loss:1.6661, D(x):0.69, D(G(z)):0.25, G(z):0.26\n",
      "Epoch [15/30], Step [1400/1875], d_loss:0.8005, g_loss:1.9266, D(x):0.71, D(G(z)):0.23, G(z):0.21\n",
      "Epoch [15/30], Step [1600/1875], d_loss:0.8448, g_loss:1.3455, D(x):0.76, D(G(z)):0.34, G(z):0.34\n",
      "Epoch [15/30], Step [1800/1875], d_loss:0.9651, g_loss:1.8299, D(x):0.68, D(G(z)):0.29, G(z):0.26\n",
      "Epoch [16/30], Step [0/1875], d_loss:1.0284, g_loss:1.8150, D(x):0.68, D(G(z)):0.29, G(z):0.27\n",
      "Epoch [16/30], Step [200/1875], d_loss:0.9873, g_loss:1.9066, D(x):0.63, D(G(z)):0.26, G(z):0.25\n",
      "Epoch [16/30], Step [400/1875], d_loss:0.7252, g_loss:1.7190, D(x):0.74, D(G(z)):0.27, G(z):0.23\n",
      "Epoch [16/30], Step [600/1875], d_loss:0.8162, g_loss:1.9713, D(x):0.68, D(G(z)):0.23, G(z):0.21\n",
      "Epoch [16/30], Step [800/1875], d_loss:0.8123, g_loss:2.0224, D(x):0.72, D(G(z)):0.23, G(z):0.19\n",
      "Epoch [16/30], Step [1000/1875], d_loss:0.7452, g_loss:1.7818, D(x):0.72, D(G(z)):0.24, G(z):0.25\n",
      "Epoch [16/30], Step [1200/1875], d_loss:0.9020, g_loss:1.3946, D(x):0.75, D(G(z)):0.34, G(z):0.32\n",
      "Epoch [16/30], Step [1400/1875], d_loss:1.1327, g_loss:1.6800, D(x):0.54, D(G(z)):0.25, G(z):0.24\n",
      "Epoch [16/30], Step [1600/1875], d_loss:1.0631, g_loss:1.3179, D(x):0.68, D(G(z)):0.36, G(z):0.35\n",
      "Epoch [16/30], Step [1800/1875], d_loss:0.6462, g_loss:1.6784, D(x):0.78, D(G(z)):0.24, G(z):0.24\n",
      "Epoch [17/30], Step [0/1875], d_loss:1.3707, g_loss:1.4108, D(x):0.52, D(G(z)):0.32, G(z):0.30\n",
      "Epoch [17/30], Step [200/1875], d_loss:1.0701, g_loss:1.5321, D(x):0.65, D(G(z)):0.32, G(z):0.33\n",
      "Epoch [17/30], Step [400/1875], d_loss:1.2009, g_loss:1.3334, D(x):0.63, D(G(z)):0.34, G(z):0.34\n",
      "Epoch [17/30], Step [600/1875], d_loss:0.6862, g_loss:1.5329, D(x):0.78, D(G(z)):0.29, G(z):0.29\n",
      "Epoch [17/30], Step [800/1875], d_loss:0.6126, g_loss:1.7439, D(x):0.77, D(G(z)):0.24, G(z):0.24\n",
      "Epoch [17/30], Step [1000/1875], d_loss:0.7801, g_loss:1.6468, D(x):0.76, D(G(z)):0.28, G(z):0.27\n",
      "Epoch [17/30], Step [1200/1875], d_loss:0.5582, g_loss:1.9589, D(x):0.77, D(G(z)):0.20, G(z):0.18\n",
      "Epoch [17/30], Step [1400/1875], d_loss:0.9667, g_loss:1.3825, D(x):0.63, D(G(z)):0.30, G(z):0.30\n",
      "Epoch [17/30], Step [1600/1875], d_loss:1.1894, g_loss:1.7706, D(x):0.49, D(G(z)):0.23, G(z):0.23\n",
      "Epoch [17/30], Step [1800/1875], d_loss:0.9555, g_loss:1.4277, D(x):0.64, D(G(z)):0.29, G(z):0.31\n",
      "Epoch [18/30], Step [0/1875], d_loss:1.0762, g_loss:1.4776, D(x):0.66, D(G(z)):0.30, G(z):0.31\n",
      "Epoch [18/30], Step [200/1875], d_loss:0.9653, g_loss:1.8160, D(x):0.64, D(G(z)):0.25, G(z):0.24\n",
      "Epoch [18/30], Step [400/1875], d_loss:0.9656, g_loss:1.3929, D(x):0.70, D(G(z)):0.31, G(z):0.32\n",
      "Epoch [18/30], Step [600/1875], d_loss:1.0476, g_loss:1.5136, D(x):0.62, D(G(z)):0.29, G(z):0.29\n",
      "Epoch [18/30], Step [800/1875], d_loss:0.6804, g_loss:2.0077, D(x):0.73, D(G(z)):0.22, G(z):0.21\n",
      "Epoch [18/30], Step [1000/1875], d_loss:0.9505, g_loss:1.7641, D(x):0.72, D(G(z)):0.35, G(z):0.27\n",
      "Epoch [18/30], Step [1200/1875], d_loss:0.7858, g_loss:1.7064, D(x):0.68, D(G(z)):0.23, G(z):0.23\n",
      "Epoch [18/30], Step [1400/1875], d_loss:1.1912, g_loss:1.3190, D(x):0.64, D(G(z)):0.38, G(z):0.33\n",
      "Epoch [18/30], Step [1600/1875], d_loss:1.2274, g_loss:1.1751, D(x):0.67, D(G(z)):0.43, G(z):0.36\n",
      "Epoch [18/30], Step [1800/1875], d_loss:0.7483, g_loss:1.8186, D(x):0.76, D(G(z)):0.26, G(z):0.27\n",
      "Epoch [19/30], Step [0/1875], d_loss:1.1848, g_loss:1.2147, D(x):0.66, D(G(z)):0.42, G(z):0.38\n",
      "Epoch [19/30], Step [200/1875], d_loss:0.8261, g_loss:1.3774, D(x):0.73, D(G(z)):0.31, G(z):0.32\n",
      "Epoch [19/30], Step [400/1875], d_loss:1.1344, g_loss:1.4885, D(x):0.58, D(G(z)):0.29, G(z):0.32\n",
      "Epoch [19/30], Step [600/1875], d_loss:0.9684, g_loss:1.5204, D(x):0.67, D(G(z)):0.31, G(z):0.31\n",
      "Epoch [19/30], Step [800/1875], d_loss:1.0475, g_loss:1.2300, D(x):0.67, D(G(z)):0.36, G(z):0.36\n",
      "Epoch [19/30], Step [1000/1875], d_loss:1.0282, g_loss:1.6550, D(x):0.62, D(G(z)):0.24, G(z):0.26\n",
      "Epoch [19/30], Step [1200/1875], d_loss:0.7587, g_loss:1.8400, D(x):0.71, D(G(z)):0.25, G(z):0.23\n",
      "Epoch [19/30], Step [1400/1875], d_loss:0.8731, g_loss:1.5729, D(x):0.73, D(G(z)):0.31, G(z):0.29\n",
      "Epoch [19/30], Step [1600/1875], d_loss:1.1719, g_loss:1.2923, D(x):0.61, D(G(z)):0.36, G(z):0.34\n",
      "Epoch [19/30], Step [1800/1875], d_loss:0.8594, g_loss:1.4603, D(x):0.71, D(G(z)):0.31, G(z):0.27\n",
      "Epoch [20/30], Step [0/1875], d_loss:1.0006, g_loss:1.7341, D(x):0.70, D(G(z)):0.31, G(z):0.26\n",
      "Epoch [20/30], Step [200/1875], d_loss:1.1606, g_loss:1.0509, D(x):0.74, D(G(z)):0.48, G(z):0.43\n",
      "Epoch [20/30], Step [400/1875], d_loss:1.1795, g_loss:1.1323, D(x):0.76, D(G(z)):0.49, G(z):0.41\n",
      "Epoch [20/30], Step [600/1875], d_loss:1.1472, g_loss:1.2623, D(x):0.71, D(G(z)):0.39, G(z):0.36\n",
      "Epoch [20/30], Step [800/1875], d_loss:0.9703, g_loss:1.3985, D(x):0.71, D(G(z)):0.36, G(z):0.34\n",
      "Epoch [20/30], Step [1000/1875], d_loss:1.0338, g_loss:1.3858, D(x):0.74, D(G(z)):0.37, G(z):0.35\n",
      "Epoch [20/30], Step [1200/1875], d_loss:1.2553, g_loss:1.2130, D(x):0.63, D(G(z)):0.41, G(z):0.38\n",
      "Epoch [20/30], Step [1400/1875], d_loss:1.1656, g_loss:1.2748, D(x):0.70, D(G(z)):0.37, G(z):0.37\n",
      "Epoch [20/30], Step [1600/1875], d_loss:0.9376, g_loss:1.3685, D(x):0.76, D(G(z)):0.38, G(z):0.33\n",
      "Epoch [20/30], Step [1800/1875], d_loss:0.9660, g_loss:1.2352, D(x):0.74, D(G(z)):0.41, G(z):0.37\n",
      "Epoch [21/30], Step [0/1875], d_loss:0.9540, g_loss:1.4007, D(x):0.66, D(G(z)):0.32, G(z):0.31\n",
      "Epoch [21/30], Step [200/1875], d_loss:0.7712, g_loss:1.5560, D(x):0.77, D(G(z)):0.32, G(z):0.28\n",
      "Epoch [21/30], Step [400/1875], d_loss:1.0685, g_loss:1.4521, D(x):0.64, D(G(z)):0.35, G(z):0.33\n",
      "Epoch [21/30], Step [600/1875], d_loss:0.8299, g_loss:1.6899, D(x):0.78, D(G(z)):0.28, G(z):0.28\n",
      "Epoch [21/30], Step [800/1875], d_loss:0.8030, g_loss:1.8045, D(x):0.69, D(G(z)):0.26, G(z):0.22\n",
      "Epoch [21/30], Step [1000/1875], d_loss:1.0310, g_loss:1.6860, D(x):0.67, D(G(z)):0.33, G(z):0.28\n",
      "Epoch [21/30], Step [1200/1875], d_loss:0.9438, g_loss:1.4294, D(x):0.72, D(G(z)):0.32, G(z):0.33\n",
      "Epoch [21/30], Step [1400/1875], d_loss:0.9655, g_loss:1.3728, D(x):0.67, D(G(z)):0.35, G(z):0.33\n",
      "Epoch [21/30], Step [1600/1875], d_loss:1.1108, g_loss:1.3012, D(x):0.68, D(G(z)):0.39, G(z):0.35\n",
      "Epoch [21/30], Step [1800/1875], d_loss:0.8390, g_loss:1.5847, D(x):0.76, D(G(z)):0.34, G(z):0.32\n",
      "Epoch [22/30], Step [0/1875], d_loss:1.1492, g_loss:1.5500, D(x):0.61, D(G(z)):0.30, G(z):0.29\n",
      "Epoch [22/30], Step [200/1875], d_loss:0.8664, g_loss:1.3653, D(x):0.77, D(G(z)):0.37, G(z):0.33\n",
      "Epoch [22/30], Step [400/1875], d_loss:1.1026, g_loss:1.7698, D(x):0.59, D(G(z)):0.27, G(z):0.24\n",
      "Epoch [22/30], Step [600/1875], d_loss:1.2049, g_loss:1.0792, D(x):0.71, D(G(z)):0.46, G(z):0.43\n",
      "Epoch [22/30], Step [800/1875], d_loss:0.9962, g_loss:1.6236, D(x):0.62, D(G(z)):0.30, G(z):0.27\n",
      "Epoch [22/30], Step [1000/1875], d_loss:0.8932, g_loss:1.5058, D(x):0.66, D(G(z)):0.27, G(z):0.28\n",
      "Epoch [22/30], Step [1200/1875], d_loss:1.0145, g_loss:1.3688, D(x):0.62, D(G(z)):0.30, G(z):0.31\n",
      "Epoch [22/30], Step [1400/1875], d_loss:0.8608, g_loss:1.7659, D(x):0.70, D(G(z)):0.28, G(z):0.24\n",
      "Epoch [22/30], Step [1600/1875], d_loss:0.9321, g_loss:1.4164, D(x):0.70, D(G(z)):0.33, G(z):0.32\n",
      "Epoch [22/30], Step [1800/1875], d_loss:0.8825, g_loss:1.6934, D(x):0.71, D(G(z)):0.31, G(z):0.26\n",
      "Epoch [23/30], Step [0/1875], d_loss:0.9165, g_loss:1.1289, D(x):0.78, D(G(z)):0.41, G(z):0.41\n",
      "Epoch [23/30], Step [200/1875], d_loss:1.1059, g_loss:1.1204, D(x):0.71, D(G(z)):0.42, G(z):0.40\n",
      "Epoch [23/30], Step [400/1875], d_loss:1.0391, g_loss:1.2347, D(x):0.73, D(G(z)):0.41, G(z):0.38\n",
      "Epoch [23/30], Step [600/1875], d_loss:0.9346, g_loss:1.4655, D(x):0.64, D(G(z)):0.29, G(z):0.28\n",
      "Epoch [23/30], Step [800/1875], d_loss:0.7501, g_loss:1.6996, D(x):0.75, D(G(z)):0.29, G(z):0.27\n",
      "Epoch [23/30], Step [1000/1875], d_loss:1.1900, g_loss:1.4654, D(x):0.56, D(G(z)):0.30, G(z):0.30\n",
      "Epoch [23/30], Step [1200/1875], d_loss:1.1358, g_loss:1.2864, D(x):0.72, D(G(z)):0.38, G(z):0.36\n",
      "Epoch [23/30], Step [1400/1875], d_loss:0.8749, g_loss:1.4936, D(x):0.67, D(G(z)):0.30, G(z):0.29\n",
      "Epoch [23/30], Step [1600/1875], d_loss:1.0289, g_loss:1.1025, D(x):0.72, D(G(z)):0.44, G(z):0.40\n",
      "Epoch [23/30], Step [1800/1875], d_loss:1.0010, g_loss:1.3238, D(x):0.67, D(G(z)):0.34, G(z):0.34\n",
      "Epoch [24/30], Step [0/1875], d_loss:0.9861, g_loss:1.4358, D(x):0.66, D(G(z)):0.33, G(z):0.30\n",
      "Epoch [24/30], Step [200/1875], d_loss:1.0976, g_loss:1.2140, D(x):0.59, D(G(z)):0.31, G(z):0.35\n",
      "Epoch [24/30], Step [400/1875], d_loss:1.0748, g_loss:1.4994, D(x):0.62, D(G(z)):0.30, G(z):0.29\n",
      "Epoch [24/30], Step [600/1875], d_loss:0.8241, g_loss:1.1877, D(x):0.81, D(G(z)):0.39, G(z):0.38\n",
      "Epoch [24/30], Step [800/1875], d_loss:0.9936, g_loss:1.3234, D(x):0.76, D(G(z)):0.41, G(z):0.37\n",
      "Epoch [24/30], Step [1000/1875], d_loss:0.9209, g_loss:1.5323, D(x):0.71, D(G(z)):0.33, G(z):0.32\n",
      "Epoch [24/30], Step [1200/1875], d_loss:0.8129, g_loss:1.6912, D(x):0.69, D(G(z)):0.26, G(z):0.26\n",
      "Epoch [24/30], Step [1400/1875], d_loss:1.0973, g_loss:1.4009, D(x):0.67, D(G(z)):0.39, G(z):0.35\n",
      "Epoch [24/30], Step [1600/1875], d_loss:0.7670, g_loss:1.5520, D(x):0.69, D(G(z)):0.26, G(z):0.26\n",
      "Epoch [24/30], Step [1800/1875], d_loss:1.0526, g_loss:1.3359, D(x):0.63, D(G(z)):0.32, G(z):0.33\n",
      "Epoch [25/30], Step [0/1875], d_loss:1.0568, g_loss:1.0042, D(x):0.74, D(G(z)):0.46, G(z):0.42\n",
      "Epoch [25/30], Step [200/1875], d_loss:0.9864, g_loss:1.4093, D(x):0.68, D(G(z)):0.35, G(z):0.32\n",
      "Epoch [25/30], Step [400/1875], d_loss:0.9382, g_loss:1.3950, D(x):0.70, D(G(z)):0.35, G(z):0.33\n",
      "Epoch [25/30], Step [600/1875], d_loss:1.1680, g_loss:1.2155, D(x):0.63, D(G(z)):0.39, G(z):0.36\n",
      "Epoch [25/30], Step [800/1875], d_loss:1.0678, g_loss:1.1406, D(x):0.74, D(G(z)):0.40, G(z):0.40\n",
      "Epoch [25/30], Step [1000/1875], d_loss:1.0081, g_loss:1.4755, D(x):0.65, D(G(z)):0.31, G(z):0.31\n",
      "Epoch [25/30], Step [1200/1875], d_loss:1.0740, g_loss:1.4499, D(x):0.68, D(G(z)):0.37, G(z):0.33\n",
      "Epoch [25/30], Step [1400/1875], d_loss:0.9066, g_loss:1.3909, D(x):0.72, D(G(z)):0.36, G(z):0.32\n",
      "Epoch [25/30], Step [1600/1875], d_loss:0.9088, g_loss:1.3628, D(x):0.72, D(G(z)):0.34, G(z):0.31\n",
      "Epoch [25/30], Step [1800/1875], d_loss:0.8421, g_loss:1.8406, D(x):0.69, D(G(z)):0.27, G(z):0.23\n",
      "Epoch [26/30], Step [0/1875], d_loss:0.8154, g_loss:1.4962, D(x):0.73, D(G(z)):0.31, G(z):0.29\n",
      "Epoch [26/30], Step [200/1875], d_loss:1.3424, g_loss:1.0003, D(x):0.68, D(G(z)):0.47, G(z):0.46\n",
      "Epoch [26/30], Step [400/1875], d_loss:0.8755, g_loss:1.3010, D(x):0.70, D(G(z)):0.32, G(z):0.31\n",
      "Epoch [26/30], Step [600/1875], d_loss:0.9245, g_loss:1.1304, D(x):0.77, D(G(z)):0.41, G(z):0.39\n",
      "Epoch [26/30], Step [800/1875], d_loss:0.9416, g_loss:1.3014, D(x):0.70, D(G(z)):0.36, G(z):0.32\n",
      "Epoch [26/30], Step [1000/1875], d_loss:0.9631, g_loss:1.4386, D(x):0.63, D(G(z)):0.31, G(z):0.31\n",
      "Epoch [26/30], Step [1200/1875], d_loss:1.0673, g_loss:1.3766, D(x):0.65, D(G(z)):0.34, G(z):0.33\n",
      "Epoch [26/30], Step [1400/1875], d_loss:0.8556, g_loss:1.7413, D(x):0.65, D(G(z)):0.27, G(z):0.24\n",
      "Epoch [26/30], Step [1600/1875], d_loss:0.9429, g_loss:1.3658, D(x):0.70, D(G(z)):0.36, G(z):0.31\n",
      "Epoch [26/30], Step [1800/1875], d_loss:1.0278, g_loss:1.3488, D(x):0.63, D(G(z)):0.35, G(z):0.33\n",
      "Epoch [27/30], Step [0/1875], d_loss:1.1390, g_loss:1.2388, D(x):0.60, D(G(z)):0.36, G(z):0.36\n",
      "Epoch [27/30], Step [200/1875], d_loss:1.1026, g_loss:1.3035, D(x):0.69, D(G(z)):0.38, G(z):0.33\n",
      "Epoch [27/30], Step [400/1875], d_loss:1.1750, g_loss:1.3565, D(x):0.61, D(G(z)):0.35, G(z):0.36\n",
      "Epoch [27/30], Step [600/1875], d_loss:0.9074, g_loss:1.4358, D(x):0.67, D(G(z)):0.31, G(z):0.29\n",
      "Epoch [27/30], Step [800/1875], d_loss:0.9269, g_loss:1.5975, D(x):0.73, D(G(z)):0.33, G(z):0.29\n",
      "Epoch [27/30], Step [1000/1875], d_loss:1.2076, g_loss:1.2675, D(x):0.56, D(G(z)):0.37, G(z):0.37\n",
      "Epoch [27/30], Step [1200/1875], d_loss:1.0054, g_loss:1.3560, D(x):0.68, D(G(z)):0.37, G(z):0.35\n",
      "Epoch [27/30], Step [1400/1875], d_loss:1.0666, g_loss:1.3214, D(x):0.63, D(G(z)):0.33, G(z):0.34\n",
      "Epoch [27/30], Step [1600/1875], d_loss:0.9891, g_loss:1.1559, D(x):0.75, D(G(z)):0.41, G(z):0.38\n",
      "Epoch [27/30], Step [1800/1875], d_loss:1.0044, g_loss:1.2597, D(x):0.70, D(G(z)):0.38, G(z):0.37\n",
      "Epoch [28/30], Step [0/1875], d_loss:0.9329, g_loss:1.3548, D(x):0.71, D(G(z)):0.37, G(z):0.33\n",
      "Epoch [28/30], Step [200/1875], d_loss:1.0556, g_loss:1.3431, D(x):0.60, D(G(z)):0.31, G(z):0.31\n",
      "Epoch [28/30], Step [400/1875], d_loss:1.0393, g_loss:1.2688, D(x):0.67, D(G(z)):0.34, G(z):0.35\n",
      "Epoch [28/30], Step [600/1875], d_loss:1.0211, g_loss:1.3348, D(x):0.63, D(G(z)):0.34, G(z):0.33\n",
      "Epoch [28/30], Step [800/1875], d_loss:1.0392, g_loss:1.1361, D(x):0.68, D(G(z)):0.39, G(z):0.39\n",
      "Epoch [28/30], Step [1000/1875], d_loss:1.2599, g_loss:1.2613, D(x):0.58, D(G(z)):0.39, G(z):0.37\n",
      "Epoch [28/30], Step [1200/1875], d_loss:0.9103, g_loss:1.2128, D(x):0.75, D(G(z)):0.37, G(z):0.39\n",
      "Epoch [28/30], Step [1400/1875], d_loss:1.1297, g_loss:1.1390, D(x):0.65, D(G(z)):0.38, G(z):0.38\n",
      "Epoch [28/30], Step [1600/1875], d_loss:0.9503, g_loss:1.3096, D(x):0.63, D(G(z)):0.31, G(z):0.31\n",
      "Epoch [28/30], Step [1800/1875], d_loss:1.1310, g_loss:0.9419, D(x):0.73, D(G(z)):0.46, G(z):0.44\n",
      "Epoch [29/30], Step [0/1875], d_loss:0.9500, g_loss:1.6640, D(x):0.59, D(G(z)):0.27, G(z):0.26\n",
      "Epoch [29/30], Step [200/1875], d_loss:0.8650, g_loss:1.3701, D(x):0.70, D(G(z)):0.33, G(z):0.30\n",
      "Epoch [29/30], Step [400/1875], d_loss:0.9671, g_loss:1.6318, D(x):0.57, D(G(z)):0.24, G(z):0.24\n",
      "Epoch [29/30], Step [600/1875], d_loss:1.0507, g_loss:1.6436, D(x):0.57, D(G(z)):0.28, G(z):0.27\n",
      "Epoch [29/30], Step [800/1875], d_loss:1.0348, g_loss:1.4222, D(x):0.65, D(G(z)):0.33, G(z):0.31\n",
      "Epoch [29/30], Step [1000/1875], d_loss:1.0120, g_loss:1.3623, D(x):0.64, D(G(z)):0.35, G(z):0.32\n",
      "Epoch [29/30], Step [1200/1875], d_loss:1.0709, g_loss:1.1645, D(x):0.66, D(G(z)):0.40, G(z):0.37\n",
      "Epoch [29/30], Step [1400/1875], d_loss:1.1865, g_loss:0.8861, D(x):0.72, D(G(z)):0.48, G(z):0.47\n",
      "Epoch [29/30], Step [1600/1875], d_loss:0.9870, g_loss:1.2394, D(x):0.73, D(G(z)):0.39, G(z):0.36\n",
      "Epoch [29/30], Step [1800/1875], d_loss:1.1275, g_loss:1.7914, D(x):0.55, D(G(z)):0.28, G(z):0.26\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(dataloader)\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(dataloader):\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.reshape(batch_size,image_size).to(device)\n",
    "        real_labels = torch.ones(batch_size,1).to(device) #生成全1的标签\n",
    "        fake_labels = torch.zeros(batch_size,1).to(device)#生成全0的标签\n",
    "        \n",
    "        ### 判别器的训练过程\n",
    "        # write your code here\n",
    "        outputs = D(images)\n",
    "        d_loss_real = loss_fn(outputs,real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        z = torch.randn(batch_size,latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images.detach())\n",
    "        d_loss_fake = loss_fn(outputs,fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()        \n",
    "        \n",
    "        \n",
    "        ### 生成器的训练过程\n",
    "        # write your code here\n",
    "        \n",
    "        outputs = D(fake_images)\n",
    "        g_loss = loss_fn(outputs,real_labels)\n",
    "        g_score = outputs\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "       \n",
    "\n",
    "        ### 输出训练过程        \n",
    "        if i % 200 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], d_loss:{:.4f}, g_loss:{:.4f}, D(x):{:.2f}, D(G(z)):{:.2f}, G(z):{:.2f}\"\n",
    "                 .format(epoch, num_epochs,i, total_steps, d_loss.item(), g_loss.item(), \n",
    "                         real_score.mean().item(), fake_score.mean().item(), g_score.mean().item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1df48dc2e88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = torch.randn(batch_size,latent_size).to(device)# 随机生成一个Batch的向量\n",
    "fake_images = G(z) # 生成该Batch向量的图像\n",
    "fake_images = fake_images.view(batch_size,28,28).data.cpu().numpy() # 将图像转化为二维，以便展示\n",
    "plt.imshow(fake_images[0],cmap=plt.cm.gray) # 挑选Batch的第一个图像，进行展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 模型存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(),\"mnist_generator.pth\")\n",
    "torch.save(D.state_dict(),\"mnist_discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1df77d13c88>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size = 28 * 28\n",
    "hidden_size = 256\n",
    "latent_size = 64\n",
    "batch_size = 32\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size,hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size,image_size),\n",
    "    nn.Tanh()    \n",
    ")\n",
    "G = G.to(device)\n",
    "G.load_state_dict(torch.load(\"mnist_generator.pth\"))\n",
    "z = torch.randn(batch_size,latent_size).to(device)\n",
    "fake_images = G(z)\n",
    "fake_images = fake_images.view(batch_size,28,28).data.cpu().numpy()\n",
    "plt.imshow(fake_images[0],cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拓展代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/938], Generator Loss: 0.7227, Discriminator Loss: 0.7054\n",
      "Epoch [1/100], Step [201/938], Generator Loss: 0.3047, Discriminator Loss: 0.8003\n",
      "Epoch [1/100], Step [401/938], Generator Loss: 0.3977, Discriminator Loss: 0.6897\n",
      "Epoch [1/100], Step [601/938], Generator Loss: 1.3082, Discriminator Loss: 0.4840\n",
      "Epoch [1/100], Step [801/938], Generator Loss: 0.9840, Discriminator Loss: 0.4842\n",
      "Epoch [2/100], Step [1/938], Generator Loss: 1.3368, Discriminator Loss: 0.4884\n",
      "Epoch [2/100], Step [201/938], Generator Loss: 1.5333, Discriminator Loss: 0.6825\n",
      "Epoch [2/100], Step [401/938], Generator Loss: 1.5726, Discriminator Loss: 0.5720\n",
      "Epoch [2/100], Step [601/938], Generator Loss: 1.4091, Discriminator Loss: 0.4837\n",
      "Epoch [2/100], Step [801/938], Generator Loss: 1.2981, Discriminator Loss: 0.4168\n",
      "Epoch [3/100], Step [1/938], Generator Loss: 0.6658, Discriminator Loss: 0.5044\n",
      "Epoch [3/100], Step [201/938], Generator Loss: 0.4927, Discriminator Loss: 0.5994\n",
      "Epoch [3/100], Step [401/938], Generator Loss: 1.3934, Discriminator Loss: 0.5576\n",
      "Epoch [3/100], Step [601/938], Generator Loss: 1.0135, Discriminator Loss: 0.4432\n",
      "Epoch [3/100], Step [801/938], Generator Loss: 1.8387, Discriminator Loss: 0.4739\n",
      "Epoch [4/100], Step [1/938], Generator Loss: 0.8761, Discriminator Loss: 0.4703\n",
      "Epoch [4/100], Step [201/938], Generator Loss: 0.6405, Discriminator Loss: 0.5090\n",
      "Epoch [4/100], Step [401/938], Generator Loss: 0.5979, Discriminator Loss: 0.6000\n",
      "Epoch [4/100], Step [601/938], Generator Loss: 0.5489, Discriminator Loss: 0.5680\n",
      "Epoch [4/100], Step [801/938], Generator Loss: 1.4007, Discriminator Loss: 0.4830\n",
      "Epoch [5/100], Step [1/938], Generator Loss: 0.8614, Discriminator Loss: 0.4850\n",
      "Epoch [5/100], Step [201/938], Generator Loss: 1.7494, Discriminator Loss: 0.5030\n",
      "Epoch [5/100], Step [401/938], Generator Loss: 2.8545, Discriminator Loss: 0.6289\n",
      "Epoch [5/100], Step [601/938], Generator Loss: 2.4990, Discriminator Loss: 0.6325\n",
      "Epoch [5/100], Step [801/938], Generator Loss: 0.9421, Discriminator Loss: 0.4285\n",
      "Epoch [6/100], Step [1/938], Generator Loss: 1.5212, Discriminator Loss: 0.3922\n",
      "Epoch [6/100], Step [201/938], Generator Loss: 1.5281, Discriminator Loss: 0.4465\n",
      "Epoch [6/100], Step [401/938], Generator Loss: 0.5813, Discriminator Loss: 0.5597\n",
      "Epoch [6/100], Step [601/938], Generator Loss: 1.3481, Discriminator Loss: 0.4129\n",
      "Epoch [6/100], Step [801/938], Generator Loss: 1.2973, Discriminator Loss: 0.3702\n",
      "Epoch [7/100], Step [1/938], Generator Loss: 0.9929, Discriminator Loss: 0.4696\n",
      "Epoch [7/100], Step [201/938], Generator Loss: 1.5198, Discriminator Loss: 0.4627\n",
      "Epoch [7/100], Step [401/938], Generator Loss: 0.7279, Discriminator Loss: 0.4807\n",
      "Epoch [7/100], Step [601/938], Generator Loss: 0.8773, Discriminator Loss: 0.4767\n",
      "Epoch [7/100], Step [801/938], Generator Loss: 1.3396, Discriminator Loss: 0.4060\n",
      "Epoch [8/100], Step [1/938], Generator Loss: 2.5600, Discriminator Loss: 0.6031\n",
      "Epoch [8/100], Step [201/938], Generator Loss: 1.0386, Discriminator Loss: 0.4501\n",
      "Epoch [8/100], Step [401/938], Generator Loss: 0.8483, Discriminator Loss: 0.3801\n",
      "Epoch [8/100], Step [601/938], Generator Loss: 0.9640, Discriminator Loss: 0.4140\n",
      "Epoch [8/100], Step [801/938], Generator Loss: 0.9229, Discriminator Loss: 0.4969\n",
      "Epoch [9/100], Step [1/938], Generator Loss: 1.5995, Discriminator Loss: 0.4145\n",
      "Epoch [9/100], Step [201/938], Generator Loss: 0.7425, Discriminator Loss: 0.4844\n",
      "Epoch [9/100], Step [401/938], Generator Loss: 0.8273, Discriminator Loss: 0.4636\n",
      "Epoch [9/100], Step [601/938], Generator Loss: 1.4220, Discriminator Loss: 0.3085\n",
      "Epoch [9/100], Step [801/938], Generator Loss: 0.9120, Discriminator Loss: 0.4130\n",
      "Epoch [10/100], Step [1/938], Generator Loss: 0.2059, Discriminator Loss: 1.0072\n",
      "Epoch [10/100], Step [201/938], Generator Loss: 1.1126, Discriminator Loss: 0.3803\n",
      "Epoch [10/100], Step [401/938], Generator Loss: 0.8129, Discriminator Loss: 0.4209\n",
      "Epoch [10/100], Step [601/938], Generator Loss: 1.2108, Discriminator Loss: 0.3078\n",
      "Epoch [10/100], Step [801/938], Generator Loss: 0.9216, Discriminator Loss: 0.4755\n",
      "Epoch [11/100], Step [1/938], Generator Loss: 1.4954, Discriminator Loss: 0.3103\n",
      "Epoch [11/100], Step [201/938], Generator Loss: 2.1261, Discriminator Loss: 0.4246\n",
      "Epoch [11/100], Step [401/938], Generator Loss: 0.9566, Discriminator Loss: 0.4739\n",
      "Epoch [11/100], Step [601/938], Generator Loss: 1.2402, Discriminator Loss: 0.4045\n",
      "Epoch [11/100], Step [801/938], Generator Loss: 1.7728, Discriminator Loss: 0.3732\n",
      "Epoch [12/100], Step [1/938], Generator Loss: 3.1665, Discriminator Loss: 0.3991\n",
      "Epoch [12/100], Step [201/938], Generator Loss: 1.4211, Discriminator Loss: 0.3382\n",
      "Epoch [12/100], Step [401/938], Generator Loss: 1.0436, Discriminator Loss: 0.3573\n",
      "Epoch [12/100], Step [601/938], Generator Loss: 1.2797, Discriminator Loss: 0.4155\n",
      "Epoch [12/100], Step [801/938], Generator Loss: 1.5620, Discriminator Loss: 0.4185\n",
      "Epoch [13/100], Step [1/938], Generator Loss: 2.4714, Discriminator Loss: 0.2930\n",
      "Epoch [13/100], Step [201/938], Generator Loss: 1.8973, Discriminator Loss: 0.2688\n",
      "Epoch [13/100], Step [401/938], Generator Loss: 1.4132, Discriminator Loss: 0.3321\n",
      "Epoch [13/100], Step [601/938], Generator Loss: 2.9323, Discriminator Loss: 0.5538\n",
      "Epoch [13/100], Step [801/938], Generator Loss: 0.4454, Discriminator Loss: 0.6341\n",
      "Epoch [14/100], Step [1/938], Generator Loss: 1.1273, Discriminator Loss: 0.3933\n",
      "Epoch [14/100], Step [201/938], Generator Loss: 1.6013, Discriminator Loss: 0.2820\n",
      "Epoch [14/100], Step [401/938], Generator Loss: 2.7549, Discriminator Loss: 0.3527\n",
      "Epoch [14/100], Step [601/938], Generator Loss: 1.2857, Discriminator Loss: 0.3815\n",
      "Epoch [14/100], Step [801/938], Generator Loss: 2.5077, Discriminator Loss: 0.3986\n",
      "Epoch [15/100], Step [1/938], Generator Loss: 1.1351, Discriminator Loss: 0.3587\n",
      "Epoch [15/100], Step [201/938], Generator Loss: 1.1339, Discriminator Loss: 0.3263\n",
      "Epoch [15/100], Step [401/938], Generator Loss: 1.4562, Discriminator Loss: 0.4256\n",
      "Epoch [15/100], Step [601/938], Generator Loss: 3.1599, Discriminator Loss: 0.4156\n",
      "Epoch [15/100], Step [801/938], Generator Loss: 1.7777, Discriminator Loss: 0.2190\n",
      "Epoch [16/100], Step [1/938], Generator Loss: 0.9262, Discriminator Loss: 0.4331\n",
      "Epoch [16/100], Step [201/938], Generator Loss: 2.5924, Discriminator Loss: 0.5069\n",
      "Epoch [16/100], Step [401/938], Generator Loss: 1.3050, Discriminator Loss: 0.3651\n",
      "Epoch [16/100], Step [601/938], Generator Loss: 2.1384, Discriminator Loss: 0.2472\n",
      "Epoch [16/100], Step [801/938], Generator Loss: 1.4492, Discriminator Loss: 0.3561\n",
      "Epoch [17/100], Step [1/938], Generator Loss: 0.5020, Discriminator Loss: 0.7342\n",
      "Epoch [17/100], Step [201/938], Generator Loss: 1.8274, Discriminator Loss: 0.3779\n",
      "Epoch [17/100], Step [401/938], Generator Loss: 1.0289, Discriminator Loss: 0.3847\n",
      "Epoch [17/100], Step [601/938], Generator Loss: 1.5472, Discriminator Loss: 0.2507\n",
      "Epoch [17/100], Step [801/938], Generator Loss: 2.5248, Discriminator Loss: 0.4003\n",
      "Epoch [18/100], Step [1/938], Generator Loss: 2.6344, Discriminator Loss: 0.6422\n",
      "Epoch [18/100], Step [201/938], Generator Loss: 2.8263, Discriminator Loss: 0.4765\n",
      "Epoch [18/100], Step [401/938], Generator Loss: 1.1901, Discriminator Loss: 0.3980\n",
      "Epoch [18/100], Step [601/938], Generator Loss: 1.5161, Discriminator Loss: 0.3069\n",
      "Epoch [18/100], Step [801/938], Generator Loss: 2.8933, Discriminator Loss: 0.5041\n",
      "Epoch [19/100], Step [1/938], Generator Loss: 2.4395, Discriminator Loss: 0.4427\n",
      "Epoch [19/100], Step [201/938], Generator Loss: 1.8590, Discriminator Loss: 0.4654\n",
      "Epoch [19/100], Step [401/938], Generator Loss: 1.4687, Discriminator Loss: 0.2565\n",
      "Epoch [19/100], Step [601/938], Generator Loss: 2.4365, Discriminator Loss: 0.3939\n",
      "Epoch [19/100], Step [801/938], Generator Loss: 2.4315, Discriminator Loss: 0.4771\n",
      "Epoch [20/100], Step [1/938], Generator Loss: 1.9711, Discriminator Loss: 0.2552\n",
      "Epoch [20/100], Step [201/938], Generator Loss: 2.1519, Discriminator Loss: 0.2625\n",
      "Epoch [20/100], Step [401/938], Generator Loss: 0.8905, Discriminator Loss: 0.4154\n",
      "Epoch [20/100], Step [601/938], Generator Loss: 1.2533, Discriminator Loss: 0.4223\n",
      "Epoch [20/100], Step [801/938], Generator Loss: 1.7173, Discriminator Loss: 0.2969\n",
      "Epoch [21/100], Step [1/938], Generator Loss: 1.8345, Discriminator Loss: 0.2718\n",
      "Epoch [21/100], Step [201/938], Generator Loss: 0.8854, Discriminator Loss: 0.4945\n",
      "Epoch [21/100], Step [401/938], Generator Loss: 1.0855, Discriminator Loss: 0.3505\n",
      "Epoch [21/100], Step [601/938], Generator Loss: 1.3065, Discriminator Loss: 0.2750\n",
      "Epoch [21/100], Step [801/938], Generator Loss: 0.4841, Discriminator Loss: 0.5982\n",
      "Epoch [22/100], Step [1/938], Generator Loss: 2.5633, Discriminator Loss: 0.4852\n",
      "Epoch [22/100], Step [201/938], Generator Loss: 1.5357, Discriminator Loss: 0.2906\n",
      "Epoch [22/100], Step [401/938], Generator Loss: 1.0696, Discriminator Loss: 0.4049\n",
      "Epoch [22/100], Step [601/938], Generator Loss: 3.3091, Discriminator Loss: 0.2739\n",
      "Epoch [22/100], Step [801/938], Generator Loss: 1.9048, Discriminator Loss: 0.3346\n",
      "Epoch [23/100], Step [1/938], Generator Loss: 1.8041, Discriminator Loss: 0.3610\n",
      "Epoch [23/100], Step [201/938], Generator Loss: 1.9594, Discriminator Loss: 0.2803\n",
      "Epoch [23/100], Step [401/938], Generator Loss: 1.3046, Discriminator Loss: 0.3413\n",
      "Epoch [23/100], Step [601/938], Generator Loss: 2.6110, Discriminator Loss: 0.2726\n",
      "Epoch [23/100], Step [801/938], Generator Loss: 2.1598, Discriminator Loss: 0.3840\n",
      "Epoch [24/100], Step [1/938], Generator Loss: 0.9937, Discriminator Loss: 0.4238\n",
      "Epoch [24/100], Step [201/938], Generator Loss: 3.8506, Discriminator Loss: 0.5311\n",
      "Epoch [24/100], Step [401/938], Generator Loss: 1.3056, Discriminator Loss: 0.2708\n",
      "Epoch [24/100], Step [601/938], Generator Loss: 1.8368, Discriminator Loss: 0.3636\n",
      "Epoch [24/100], Step [801/938], Generator Loss: 1.0696, Discriminator Loss: 0.3905\n",
      "Epoch [25/100], Step [1/938], Generator Loss: 1.2893, Discriminator Loss: 0.3464\n",
      "Epoch [25/100], Step [201/938], Generator Loss: 1.7996, Discriminator Loss: 0.2843\n",
      "Epoch [25/100], Step [401/938], Generator Loss: 1.1378, Discriminator Loss: 0.3218\n",
      "Epoch [25/100], Step [601/938], Generator Loss: 2.5016, Discriminator Loss: 0.3404\n",
      "Epoch [25/100], Step [801/938], Generator Loss: 2.1981, Discriminator Loss: 0.3326\n",
      "Epoch [26/100], Step [1/938], Generator Loss: 2.1046, Discriminator Loss: 0.2111\n",
      "Epoch [26/100], Step [201/938], Generator Loss: 1.3293, Discriminator Loss: 0.2977\n",
      "Epoch [26/100], Step [401/938], Generator Loss: 1.6208, Discriminator Loss: 0.2605\n",
      "Epoch [26/100], Step [601/938], Generator Loss: 1.6514, Discriminator Loss: 0.3018\n",
      "Epoch [26/100], Step [801/938], Generator Loss: 1.6886, Discriminator Loss: 0.2587\n",
      "Epoch [27/100], Step [1/938], Generator Loss: 0.8835, Discriminator Loss: 0.4894\n",
      "Epoch [27/100], Step [201/938], Generator Loss: 1.3465, Discriminator Loss: 0.2555\n",
      "Epoch [27/100], Step [401/938], Generator Loss: 1.3966, Discriminator Loss: 0.3886\n",
      "Epoch [27/100], Step [601/938], Generator Loss: 2.9321, Discriminator Loss: 0.1983\n",
      "Epoch [27/100], Step [801/938], Generator Loss: 0.9320, Discriminator Loss: 0.3849\n",
      "Epoch [28/100], Step [1/938], Generator Loss: 1.8089, Discriminator Loss: 0.2412\n",
      "Epoch [28/100], Step [201/938], Generator Loss: 0.8289, Discriminator Loss: 0.4509\n",
      "Epoch [28/100], Step [401/938], Generator Loss: 1.6249, Discriminator Loss: 0.3109\n",
      "Epoch [28/100], Step [601/938], Generator Loss: 2.0441, Discriminator Loss: 0.3473\n",
      "Epoch [28/100], Step [801/938], Generator Loss: 2.2407, Discriminator Loss: 0.2407\n",
      "Epoch [29/100], Step [1/938], Generator Loss: 1.5188, Discriminator Loss: 0.2857\n",
      "Epoch [29/100], Step [201/938], Generator Loss: 2.6593, Discriminator Loss: 0.3289\n",
      "Epoch [29/100], Step [401/938], Generator Loss: 3.5209, Discriminator Loss: 0.3540\n",
      "Epoch [29/100], Step [601/938], Generator Loss: 0.6213, Discriminator Loss: 0.4988\n",
      "Epoch [29/100], Step [801/938], Generator Loss: 2.4302, Discriminator Loss: 0.3039\n",
      "Epoch [30/100], Step [1/938], Generator Loss: 1.7774, Discriminator Loss: 0.4114\n",
      "Epoch [30/100], Step [201/938], Generator Loss: 1.5922, Discriminator Loss: 0.2710\n",
      "Epoch [30/100], Step [401/938], Generator Loss: 1.7831, Discriminator Loss: 0.2889\n",
      "Epoch [30/100], Step [601/938], Generator Loss: 2.2831, Discriminator Loss: 0.3012\n",
      "Epoch [30/100], Step [801/938], Generator Loss: 1.4233, Discriminator Loss: 0.3304\n",
      "Epoch [31/100], Step [1/938], Generator Loss: 1.8396, Discriminator Loss: 0.2418\n",
      "Epoch [31/100], Step [201/938], Generator Loss: 2.0357, Discriminator Loss: 0.2868\n",
      "Epoch [31/100], Step [401/938], Generator Loss: 0.9458, Discriminator Loss: 0.4843\n",
      "Epoch [31/100], Step [601/938], Generator Loss: 3.4420, Discriminator Loss: 0.4175\n",
      "Epoch [31/100], Step [801/938], Generator Loss: 1.7204, Discriminator Loss: 0.2266\n",
      "Epoch [32/100], Step [1/938], Generator Loss: 1.8689, Discriminator Loss: 0.3062\n",
      "Epoch [32/100], Step [201/938], Generator Loss: 2.6509, Discriminator Loss: 0.3401\n",
      "Epoch [32/100], Step [401/938], Generator Loss: 1.3398, Discriminator Loss: 0.4034\n",
      "Epoch [32/100], Step [601/938], Generator Loss: 4.5850, Discriminator Loss: 0.3372\n",
      "Epoch [32/100], Step [801/938], Generator Loss: 2.3204, Discriminator Loss: 0.3116\n",
      "Epoch [33/100], Step [1/938], Generator Loss: 2.9287, Discriminator Loss: 0.3223\n",
      "Epoch [33/100], Step [201/938], Generator Loss: 1.4661, Discriminator Loss: 0.2764\n",
      "Epoch [33/100], Step [401/938], Generator Loss: 1.7345, Discriminator Loss: 0.2726\n",
      "Epoch [33/100], Step [601/938], Generator Loss: 2.6077, Discriminator Loss: 0.1947\n",
      "Epoch [33/100], Step [801/938], Generator Loss: 2.3148, Discriminator Loss: 0.2938\n",
      "Epoch [34/100], Step [1/938], Generator Loss: 1.7355, Discriminator Loss: 0.3262\n",
      "Epoch [34/100], Step [201/938], Generator Loss: 2.5940, Discriminator Loss: 0.3365\n",
      "Epoch [34/100], Step [401/938], Generator Loss: 2.8792, Discriminator Loss: 0.2313\n",
      "Epoch [34/100], Step [601/938], Generator Loss: 2.4938, Discriminator Loss: 0.2412\n",
      "Epoch [34/100], Step [801/938], Generator Loss: 1.8177, Discriminator Loss: 0.2874\n",
      "Epoch [35/100], Step [1/938], Generator Loss: 1.1882, Discriminator Loss: 0.4787\n",
      "Epoch [35/100], Step [201/938], Generator Loss: 1.0092, Discriminator Loss: 0.4632\n",
      "Epoch [35/100], Step [401/938], Generator Loss: 2.1780, Discriminator Loss: 0.3199\n",
      "Epoch [35/100], Step [601/938], Generator Loss: 4.3845, Discriminator Loss: 0.4316\n",
      "Epoch [35/100], Step [801/938], Generator Loss: 1.9062, Discriminator Loss: 0.2623\n",
      "Epoch [36/100], Step [1/938], Generator Loss: 4.3311, Discriminator Loss: 0.5135\n",
      "Epoch [36/100], Step [201/938], Generator Loss: 2.4754, Discriminator Loss: 0.2576\n",
      "Epoch [36/100], Step [401/938], Generator Loss: 2.2376, Discriminator Loss: 0.2049\n",
      "Epoch [36/100], Step [601/938], Generator Loss: 3.4277, Discriminator Loss: 0.4356\n",
      "Epoch [36/100], Step [801/938], Generator Loss: 2.9852, Discriminator Loss: 0.2010\n",
      "Epoch [37/100], Step [1/938], Generator Loss: 2.3594, Discriminator Loss: 0.2344\n",
      "Epoch [37/100], Step [201/938], Generator Loss: 4.5641, Discriminator Loss: 0.4445\n",
      "Epoch [37/100], Step [401/938], Generator Loss: 2.9344, Discriminator Loss: 0.3093\n",
      "Epoch [37/100], Step [601/938], Generator Loss: 2.4572, Discriminator Loss: 0.1690\n",
      "Epoch [37/100], Step [801/938], Generator Loss: 3.6627, Discriminator Loss: 0.2479\n",
      "Epoch [38/100], Step [1/938], Generator Loss: 2.2844, Discriminator Loss: 0.1951\n",
      "Epoch [38/100], Step [201/938], Generator Loss: 2.8316, Discriminator Loss: 0.2192\n",
      "Epoch [38/100], Step [401/938], Generator Loss: 2.2934, Discriminator Loss: 0.2371\n",
      "Epoch [38/100], Step [601/938], Generator Loss: 2.3536, Discriminator Loss: 0.3546\n",
      "Epoch [38/100], Step [801/938], Generator Loss: 1.8776, Discriminator Loss: 0.2749\n",
      "Epoch [39/100], Step [1/938], Generator Loss: 1.4441, Discriminator Loss: 0.3201\n",
      "Epoch [39/100], Step [201/938], Generator Loss: 3.1956, Discriminator Loss: 0.3873\n",
      "Epoch [39/100], Step [401/938], Generator Loss: 3.1150, Discriminator Loss: 0.2912\n",
      "Epoch [39/100], Step [601/938], Generator Loss: 2.5073, Discriminator Loss: 0.3204\n",
      "Epoch [39/100], Step [801/938], Generator Loss: 2.1566, Discriminator Loss: 0.3040\n",
      "Epoch [40/100], Step [1/938], Generator Loss: 1.6283, Discriminator Loss: 0.2636\n",
      "Epoch [40/100], Step [201/938], Generator Loss: 1.3911, Discriminator Loss: 0.2730\n",
      "Epoch [40/100], Step [401/938], Generator Loss: 1.8426, Discriminator Loss: 0.2921\n",
      "Epoch [40/100], Step [601/938], Generator Loss: 1.4498, Discriminator Loss: 0.3403\n",
      "Epoch [40/100], Step [801/938], Generator Loss: 1.7552, Discriminator Loss: 0.3528\n",
      "Epoch [41/100], Step [1/938], Generator Loss: 1.5046, Discriminator Loss: 0.3019\n",
      "Epoch [41/100], Step [201/938], Generator Loss: 1.3567, Discriminator Loss: 0.3621\n",
      "Epoch [41/100], Step [401/938], Generator Loss: 1.2666, Discriminator Loss: 0.3198\n",
      "Epoch [41/100], Step [601/938], Generator Loss: 2.0435, Discriminator Loss: 0.3101\n",
      "Epoch [41/100], Step [801/938], Generator Loss: 2.0593, Discriminator Loss: 0.2688\n",
      "Epoch [42/100], Step [1/938], Generator Loss: 2.0725, Discriminator Loss: 0.3060\n",
      "Epoch [42/100], Step [201/938], Generator Loss: 1.4721, Discriminator Loss: 0.2993\n",
      "Epoch [42/100], Step [401/938], Generator Loss: 1.3493, Discriminator Loss: 0.2688\n",
      "Epoch [42/100], Step [601/938], Generator Loss: 1.6219, Discriminator Loss: 0.3119\n",
      "Epoch [42/100], Step [801/938], Generator Loss: 1.6241, Discriminator Loss: 0.2407\n",
      "Epoch [43/100], Step [1/938], Generator Loss: 2.5959, Discriminator Loss: 0.4054\n",
      "Epoch [43/100], Step [201/938], Generator Loss: 4.3716, Discriminator Loss: 0.5571\n",
      "Epoch [43/100], Step [401/938], Generator Loss: 2.0656, Discriminator Loss: 0.2955\n",
      "Epoch [43/100], Step [601/938], Generator Loss: 3.0355, Discriminator Loss: 0.2257\n",
      "Epoch [43/100], Step [801/938], Generator Loss: 4.2850, Discriminator Loss: 0.3728\n",
      "Epoch [44/100], Step [1/938], Generator Loss: 3.2337, Discriminator Loss: 0.2816\n",
      "Epoch [44/100], Step [201/938], Generator Loss: 3.4244, Discriminator Loss: 0.3264\n",
      "Epoch [44/100], Step [401/938], Generator Loss: 2.0616, Discriminator Loss: 0.3657\n",
      "Epoch [44/100], Step [601/938], Generator Loss: 1.9933, Discriminator Loss: 0.3409\n",
      "Epoch [44/100], Step [801/938], Generator Loss: 1.9401, Discriminator Loss: 0.3036\n",
      "Epoch [45/100], Step [1/938], Generator Loss: 1.9828, Discriminator Loss: 0.2716\n",
      "Epoch [45/100], Step [201/938], Generator Loss: 1.4602, Discriminator Loss: 0.2455\n",
      "Epoch [45/100], Step [401/938], Generator Loss: 3.4435, Discriminator Loss: 0.2170\n",
      "Epoch [45/100], Step [601/938], Generator Loss: 3.4213, Discriminator Loss: 0.3011\n",
      "Epoch [45/100], Step [801/938], Generator Loss: 2.1595, Discriminator Loss: 0.2413\n",
      "Epoch [46/100], Step [1/938], Generator Loss: 1.3732, Discriminator Loss: 0.2866\n",
      "Epoch [46/100], Step [201/938], Generator Loss: 1.9236, Discriminator Loss: 0.1916\n",
      "Epoch [46/100], Step [401/938], Generator Loss: 4.5454, Discriminator Loss: 0.2156\n",
      "Epoch [46/100], Step [601/938], Generator Loss: 2.2332, Discriminator Loss: 0.2687\n",
      "Epoch [46/100], Step [801/938], Generator Loss: 2.3189, Discriminator Loss: 0.2260\n",
      "Epoch [47/100], Step [1/938], Generator Loss: 1.3654, Discriminator Loss: 0.3032\n",
      "Epoch [47/100], Step [201/938], Generator Loss: 2.1493, Discriminator Loss: 0.2844\n",
      "Epoch [47/100], Step [401/938], Generator Loss: 2.3501, Discriminator Loss: 0.2873\n",
      "Epoch [47/100], Step [601/938], Generator Loss: 1.3277, Discriminator Loss: 0.5156\n",
      "Epoch [47/100], Step [801/938], Generator Loss: 2.2981, Discriminator Loss: 0.3069\n",
      "Epoch [48/100], Step [1/938], Generator Loss: 1.0242, Discriminator Loss: 0.3603\n",
      "Epoch [48/100], Step [201/938], Generator Loss: 1.8940, Discriminator Loss: 0.2862\n",
      "Epoch [48/100], Step [401/938], Generator Loss: 1.7131, Discriminator Loss: 0.2060\n",
      "Epoch [48/100], Step [601/938], Generator Loss: 2.0636, Discriminator Loss: 0.2696\n",
      "Epoch [48/100], Step [801/938], Generator Loss: 1.3010, Discriminator Loss: 0.3343\n",
      "Epoch [49/100], Step [1/938], Generator Loss: 2.3515, Discriminator Loss: 0.2056\n",
      "Epoch [49/100], Step [201/938], Generator Loss: 2.5243, Discriminator Loss: 0.2646\n",
      "Epoch [49/100], Step [401/938], Generator Loss: 1.9723, Discriminator Loss: 0.2490\n",
      "Epoch [49/100], Step [601/938], Generator Loss: 3.0338, Discriminator Loss: 0.3196\n",
      "Epoch [49/100], Step [801/938], Generator Loss: 1.9889, Discriminator Loss: 0.2774\n",
      "Epoch [50/100], Step [1/938], Generator Loss: 0.7648, Discriminator Loss: 0.4938\n",
      "Epoch [50/100], Step [201/938], Generator Loss: 3.3707, Discriminator Loss: 0.3255\n",
      "Epoch [50/100], Step [401/938], Generator Loss: 2.5851, Discriminator Loss: 0.2626\n",
      "Epoch [50/100], Step [601/938], Generator Loss: 2.2132, Discriminator Loss: 0.1785\n",
      "Epoch [50/100], Step [801/938], Generator Loss: 3.3628, Discriminator Loss: 0.3668\n",
      "Epoch [51/100], Step [1/938], Generator Loss: 3.6460, Discriminator Loss: 0.4684\n",
      "Epoch [51/100], Step [201/938], Generator Loss: 3.3308, Discriminator Loss: 0.2837\n",
      "Epoch [51/100], Step [401/938], Generator Loss: 1.7992, Discriminator Loss: 0.4670\n",
      "Epoch [51/100], Step [601/938], Generator Loss: 2.5368, Discriminator Loss: 0.3193\n",
      "Epoch [51/100], Step [801/938], Generator Loss: 2.6184, Discriminator Loss: 0.2890\n",
      "Epoch [52/100], Step [1/938], Generator Loss: 2.5984, Discriminator Loss: 0.3142\n",
      "Epoch [52/100], Step [201/938], Generator Loss: 1.9751, Discriminator Loss: 0.1984\n",
      "Epoch [52/100], Step [401/938], Generator Loss: 3.2862, Discriminator Loss: 0.2424\n",
      "Epoch [52/100], Step [601/938], Generator Loss: 3.3916, Discriminator Loss: 0.2957\n",
      "Epoch [52/100], Step [801/938], Generator Loss: 2.5202, Discriminator Loss: 0.2288\n",
      "Epoch [53/100], Step [1/938], Generator Loss: 2.8200, Discriminator Loss: 0.1763\n",
      "Epoch [53/100], Step [201/938], Generator Loss: 0.8817, Discriminator Loss: 0.4675\n",
      "Epoch [53/100], Step [401/938], Generator Loss: 1.6340, Discriminator Loss: 0.2197\n",
      "Epoch [53/100], Step [601/938], Generator Loss: 2.0315, Discriminator Loss: 0.3115\n",
      "Epoch [53/100], Step [801/938], Generator Loss: 4.4311, Discriminator Loss: 0.3121\n",
      "Epoch [54/100], Step [1/938], Generator Loss: 3.1020, Discriminator Loss: 0.3092\n",
      "Epoch [54/100], Step [201/938], Generator Loss: 2.2895, Discriminator Loss: 0.1780\n",
      "Epoch [54/100], Step [401/938], Generator Loss: 3.5755, Discriminator Loss: 0.2548\n",
      "Epoch [54/100], Step [601/938], Generator Loss: 3.7390, Discriminator Loss: 0.4995\n",
      "Epoch [54/100], Step [801/938], Generator Loss: 2.2663, Discriminator Loss: 0.2916\n",
      "Epoch [55/100], Step [1/938], Generator Loss: 2.1959, Discriminator Loss: 0.2059\n",
      "Epoch [55/100], Step [201/938], Generator Loss: 3.7462, Discriminator Loss: 0.3776\n",
      "Epoch [55/100], Step [401/938], Generator Loss: 2.0804, Discriminator Loss: 0.3322\n",
      "Epoch [55/100], Step [601/938], Generator Loss: 2.6716, Discriminator Loss: 0.2449\n",
      "Epoch [55/100], Step [801/938], Generator Loss: 3.4821, Discriminator Loss: 0.2850\n",
      "Epoch [56/100], Step [1/938], Generator Loss: 1.3794, Discriminator Loss: 0.4120\n",
      "Epoch [56/100], Step [201/938], Generator Loss: 1.9310, Discriminator Loss: 0.1793\n",
      "Epoch [56/100], Step [401/938], Generator Loss: 2.6970, Discriminator Loss: 0.3267\n",
      "Epoch [56/100], Step [601/938], Generator Loss: 2.2622, Discriminator Loss: 0.2053\n",
      "Epoch [56/100], Step [801/938], Generator Loss: 3.2785, Discriminator Loss: 0.1381\n",
      "Epoch [57/100], Step [1/938], Generator Loss: 2.8931, Discriminator Loss: 0.1707\n",
      "Epoch [57/100], Step [201/938], Generator Loss: 4.3403, Discriminator Loss: 0.4461\n",
      "Epoch [57/100], Step [401/938], Generator Loss: 3.3357, Discriminator Loss: 0.2143\n",
      "Epoch [57/100], Step [601/938], Generator Loss: 2.4379, Discriminator Loss: 0.3406\n",
      "Epoch [57/100], Step [801/938], Generator Loss: 2.2206, Discriminator Loss: 0.1815\n",
      "Epoch [58/100], Step [1/938], Generator Loss: 1.3567, Discriminator Loss: 0.4248\n",
      "Epoch [58/100], Step [201/938], Generator Loss: 4.4584, Discriminator Loss: 0.5854\n",
      "Epoch [58/100], Step [401/938], Generator Loss: 1.3040, Discriminator Loss: 0.3802\n",
      "Epoch [58/100], Step [601/938], Generator Loss: 2.9820, Discriminator Loss: 0.2264\n",
      "Epoch [58/100], Step [801/938], Generator Loss: 2.5079, Discriminator Loss: 0.3065\n",
      "Epoch [59/100], Step [1/938], Generator Loss: 1.2395, Discriminator Loss: 0.3014\n",
      "Epoch [59/100], Step [201/938], Generator Loss: 1.8481, Discriminator Loss: 0.2582\n",
      "Epoch [59/100], Step [401/938], Generator Loss: 1.6032, Discriminator Loss: 0.2718\n",
      "Epoch [59/100], Step [601/938], Generator Loss: 3.3018, Discriminator Loss: 0.2493\n",
      "Epoch [59/100], Step [801/938], Generator Loss: 1.0554, Discriminator Loss: 0.5568\n",
      "Epoch [60/100], Step [1/938], Generator Loss: 1.8199, Discriminator Loss: 0.3020\n",
      "Epoch [60/100], Step [201/938], Generator Loss: 2.2977, Discriminator Loss: 0.1581\n",
      "Epoch [60/100], Step [401/938], Generator Loss: 1.5558, Discriminator Loss: 0.2623\n",
      "Epoch [60/100], Step [601/938], Generator Loss: 2.2303, Discriminator Loss: 0.3343\n",
      "Epoch [60/100], Step [801/938], Generator Loss: 1.5984, Discriminator Loss: 0.2470\n",
      "Epoch [61/100], Step [1/938], Generator Loss: 2.6272, Discriminator Loss: 0.4187\n",
      "Epoch [61/100], Step [201/938], Generator Loss: 1.3584, Discriminator Loss: 0.3561\n",
      "Epoch [61/100], Step [401/938], Generator Loss: 1.9797, Discriminator Loss: 0.3214\n",
      "Epoch [61/100], Step [601/938], Generator Loss: 2.3490, Discriminator Loss: 0.2600\n",
      "Epoch [61/100], Step [801/938], Generator Loss: 1.8963, Discriminator Loss: 0.3198\n",
      "Epoch [62/100], Step [1/938], Generator Loss: 2.3259, Discriminator Loss: 0.2388\n",
      "Epoch [62/100], Step [201/938], Generator Loss: 2.0585, Discriminator Loss: 0.2398\n",
      "Epoch [62/100], Step [401/938], Generator Loss: 1.2354, Discriminator Loss: 0.3835\n",
      "Epoch [62/100], Step [601/938], Generator Loss: 3.0151, Discriminator Loss: 0.4394\n",
      "Epoch [62/100], Step [801/938], Generator Loss: 3.8938, Discriminator Loss: 0.3297\n",
      "Epoch [63/100], Step [1/938], Generator Loss: 2.3766, Discriminator Loss: 0.3061\n",
      "Epoch [63/100], Step [201/938], Generator Loss: 2.2116, Discriminator Loss: 0.3372\n",
      "Epoch [63/100], Step [401/938], Generator Loss: 1.7903, Discriminator Loss: 0.2015\n",
      "Epoch [63/100], Step [601/938], Generator Loss: 1.6811, Discriminator Loss: 0.2256\n",
      "Epoch [63/100], Step [801/938], Generator Loss: 2.4600, Discriminator Loss: 0.3024\n",
      "Epoch [64/100], Step [1/938], Generator Loss: 3.9622, Discriminator Loss: 0.3426\n",
      "Epoch [64/100], Step [201/938], Generator Loss: 2.9409, Discriminator Loss: 0.2759\n",
      "Epoch [64/100], Step [401/938], Generator Loss: 2.3558, Discriminator Loss: 0.2319\n",
      "Epoch [64/100], Step [601/938], Generator Loss: 2.7132, Discriminator Loss: 0.2644\n",
      "Epoch [64/100], Step [801/938], Generator Loss: 2.3011, Discriminator Loss: 0.1466\n",
      "Epoch [65/100], Step [1/938], Generator Loss: 1.5196, Discriminator Loss: 0.3529\n",
      "Epoch [65/100], Step [201/938], Generator Loss: 2.3006, Discriminator Loss: 0.3809\n",
      "Epoch [65/100], Step [401/938], Generator Loss: 1.5978, Discriminator Loss: 0.2701\n",
      "Epoch [65/100], Step [601/938], Generator Loss: 2.2736, Discriminator Loss: 0.2004\n",
      "Epoch [65/100], Step [801/938], Generator Loss: 2.4357, Discriminator Loss: 0.2292\n",
      "Epoch [66/100], Step [1/938], Generator Loss: 2.3501, Discriminator Loss: 0.1679\n",
      "Epoch [66/100], Step [201/938], Generator Loss: 2.7226, Discriminator Loss: 0.2891\n",
      "Epoch [66/100], Step [401/938], Generator Loss: 3.9263, Discriminator Loss: 0.1374\n",
      "Epoch [66/100], Step [601/938], Generator Loss: 1.6840, Discriminator Loss: 0.2734\n",
      "Epoch [66/100], Step [801/938], Generator Loss: 2.5577, Discriminator Loss: 0.3527\n",
      "Epoch [67/100], Step [1/938], Generator Loss: 1.8156, Discriminator Loss: 0.3074\n",
      "Epoch [67/100], Step [201/938], Generator Loss: 2.4157, Discriminator Loss: 0.1657\n",
      "Epoch [67/100], Step [401/938], Generator Loss: 2.2515, Discriminator Loss: 0.1791\n",
      "Epoch [67/100], Step [601/938], Generator Loss: 1.5659, Discriminator Loss: 0.3336\n",
      "Epoch [67/100], Step [801/938], Generator Loss: 1.2940, Discriminator Loss: 0.4588\n",
      "Epoch [68/100], Step [1/938], Generator Loss: 1.1986, Discriminator Loss: 0.3770\n",
      "Epoch [68/100], Step [201/938], Generator Loss: 2.3116, Discriminator Loss: 0.2612\n",
      "Epoch [68/100], Step [401/938], Generator Loss: 3.3145, Discriminator Loss: 0.2899\n",
      "Epoch [68/100], Step [601/938], Generator Loss: 1.9065, Discriminator Loss: 0.2857\n",
      "Epoch [68/100], Step [801/938], Generator Loss: 2.0747, Discriminator Loss: 0.2338\n",
      "Epoch [69/100], Step [1/938], Generator Loss: 3.2595, Discriminator Loss: 0.2835\n",
      "Epoch [69/100], Step [201/938], Generator Loss: 2.3248, Discriminator Loss: 0.3542\n",
      "Epoch [69/100], Step [401/938], Generator Loss: 1.4349, Discriminator Loss: 0.3542\n",
      "Epoch [69/100], Step [601/938], Generator Loss: 2.4207, Discriminator Loss: 0.1689\n",
      "Epoch [69/100], Step [801/938], Generator Loss: 2.6027, Discriminator Loss: 0.2608\n",
      "Epoch [70/100], Step [1/938], Generator Loss: 2.5907, Discriminator Loss: 0.2594\n",
      "Epoch [70/100], Step [201/938], Generator Loss: 3.0497, Discriminator Loss: 0.3063\n",
      "Epoch [70/100], Step [401/938], Generator Loss: 2.8705, Discriminator Loss: 0.2117\n",
      "Epoch [70/100], Step [601/938], Generator Loss: 2.6092, Discriminator Loss: 0.3826\n",
      "Epoch [70/100], Step [801/938], Generator Loss: 2.4565, Discriminator Loss: 0.3320\n",
      "Epoch [71/100], Step [1/938], Generator Loss: 2.4426, Discriminator Loss: 0.4340\n",
      "Epoch [71/100], Step [201/938], Generator Loss: 3.3999, Discriminator Loss: 0.4198\n",
      "Epoch [71/100], Step [401/938], Generator Loss: 1.7689, Discriminator Loss: 0.4167\n",
      "Epoch [71/100], Step [601/938], Generator Loss: 1.5948, Discriminator Loss: 0.2458\n",
      "Epoch [71/100], Step [801/938], Generator Loss: 1.6158, Discriminator Loss: 0.4474\n",
      "Epoch [72/100], Step [1/938], Generator Loss: 2.6797, Discriminator Loss: 0.2505\n",
      "Epoch [72/100], Step [201/938], Generator Loss: 2.4210, Discriminator Loss: 0.2357\n",
      "Epoch [72/100], Step [401/938], Generator Loss: 3.3515, Discriminator Loss: 0.3119\n",
      "Epoch [72/100], Step [601/938], Generator Loss: 2.3940, Discriminator Loss: 0.1869\n",
      "Epoch [72/100], Step [801/938], Generator Loss: 1.1536, Discriminator Loss: 0.3890\n",
      "Epoch [73/100], Step [1/938], Generator Loss: 2.8930, Discriminator Loss: 0.2503\n",
      "Epoch [73/100], Step [201/938], Generator Loss: 1.6541, Discriminator Loss: 0.3226\n",
      "Epoch [73/100], Step [401/938], Generator Loss: 1.8278, Discriminator Loss: 0.2511\n",
      "Epoch [73/100], Step [601/938], Generator Loss: 3.4201, Discriminator Loss: 0.3997\n",
      "Epoch [73/100], Step [801/938], Generator Loss: 2.9201, Discriminator Loss: 0.2070\n",
      "Epoch [74/100], Step [1/938], Generator Loss: 2.1332, Discriminator Loss: 0.2361\n",
      "Epoch [74/100], Step [201/938], Generator Loss: 1.7355, Discriminator Loss: 0.3301\n",
      "Epoch [74/100], Step [401/938], Generator Loss: 2.7093, Discriminator Loss: 0.4133\n",
      "Epoch [74/100], Step [601/938], Generator Loss: 2.1068, Discriminator Loss: 0.3359\n",
      "Epoch [74/100], Step [801/938], Generator Loss: 2.9749, Discriminator Loss: 0.2127\n",
      "Epoch [75/100], Step [1/938], Generator Loss: 2.1175, Discriminator Loss: 0.3991\n",
      "Epoch [75/100], Step [201/938], Generator Loss: 1.8080, Discriminator Loss: 0.3009\n",
      "Epoch [75/100], Step [401/938], Generator Loss: 1.6093, Discriminator Loss: 0.4225\n",
      "Epoch [75/100], Step [601/938], Generator Loss: 1.5944, Discriminator Loss: 0.3788\n",
      "Epoch [75/100], Step [801/938], Generator Loss: 1.9772, Discriminator Loss: 0.3384\n",
      "Epoch [76/100], Step [1/938], Generator Loss: 2.0479, Discriminator Loss: 0.2717\n",
      "Epoch [76/100], Step [201/938], Generator Loss: 2.0034, Discriminator Loss: 0.2702\n",
      "Epoch [76/100], Step [401/938], Generator Loss: 2.4435, Discriminator Loss: 0.2753\n",
      "Epoch [76/100], Step [601/938], Generator Loss: 2.6487, Discriminator Loss: 0.1606\n",
      "Epoch [76/100], Step [801/938], Generator Loss: 2.5944, Discriminator Loss: 0.2850\n",
      "Epoch [77/100], Step [1/938], Generator Loss: 2.8541, Discriminator Loss: 0.2833\n",
      "Epoch [77/100], Step [201/938], Generator Loss: 2.5359, Discriminator Loss: 0.3040\n",
      "Epoch [77/100], Step [401/938], Generator Loss: 2.0421, Discriminator Loss: 0.3202\n",
      "Epoch [77/100], Step [601/938], Generator Loss: 2.3786, Discriminator Loss: 0.3200\n",
      "Epoch [77/100], Step [801/938], Generator Loss: 2.1421, Discriminator Loss: 0.2663\n",
      "Epoch [78/100], Step [1/938], Generator Loss: 2.1079, Discriminator Loss: 0.1783\n",
      "Epoch [78/100], Step [201/938], Generator Loss: 2.1040, Discriminator Loss: 0.1920\n",
      "Epoch [78/100], Step [401/938], Generator Loss: 1.8420, Discriminator Loss: 0.3042\n",
      "Epoch [78/100], Step [601/938], Generator Loss: 1.8670, Discriminator Loss: 0.2304\n",
      "Epoch [78/100], Step [801/938], Generator Loss: 2.3410, Discriminator Loss: 0.1932\n",
      "Epoch [79/100], Step [1/938], Generator Loss: 2.3364, Discriminator Loss: 0.2750\n",
      "Epoch [79/100], Step [201/938], Generator Loss: 2.7464, Discriminator Loss: 0.2731\n",
      "Epoch [79/100], Step [401/938], Generator Loss: 2.0181, Discriminator Loss: 0.2684\n",
      "Epoch [79/100], Step [601/938], Generator Loss: 1.9957, Discriminator Loss: 0.2163\n",
      "Epoch [79/100], Step [801/938], Generator Loss: 2.0861, Discriminator Loss: 0.2418\n",
      "Epoch [80/100], Step [1/938], Generator Loss: 5.3265, Discriminator Loss: 0.4092\n",
      "Epoch [80/100], Step [201/938], Generator Loss: 3.1780, Discriminator Loss: 0.2560\n",
      "Epoch [80/100], Step [401/938], Generator Loss: 2.5969, Discriminator Loss: 0.3451\n",
      "Epoch [80/100], Step [601/938], Generator Loss: 2.6411, Discriminator Loss: 0.2322\n",
      "Epoch [80/100], Step [801/938], Generator Loss: 1.6379, Discriminator Loss: 0.2570\n",
      "Epoch [81/100], Step [1/938], Generator Loss: 6.2280, Discriminator Loss: 1.0001\n",
      "Epoch [81/100], Step [201/938], Generator Loss: 2.6612, Discriminator Loss: 0.1925\n",
      "Epoch [81/100], Step [401/938], Generator Loss: 2.0031, Discriminator Loss: 0.1658\n",
      "Epoch [81/100], Step [601/938], Generator Loss: 2.3832, Discriminator Loss: 0.2573\n",
      "Epoch [81/100], Step [801/938], Generator Loss: 2.0605, Discriminator Loss: 0.2857\n",
      "Epoch [82/100], Step [1/938], Generator Loss: 1.9297, Discriminator Loss: 0.2853\n",
      "Epoch [82/100], Step [201/938], Generator Loss: 1.5951, Discriminator Loss: 0.3360\n",
      "Epoch [82/100], Step [401/938], Generator Loss: 2.1269, Discriminator Loss: 0.2211\n",
      "Epoch [82/100], Step [601/938], Generator Loss: 2.4283, Discriminator Loss: 0.1725\n",
      "Epoch [82/100], Step [801/938], Generator Loss: 2.9120, Discriminator Loss: 0.2737\n",
      "Epoch [83/100], Step [1/938], Generator Loss: 1.2940, Discriminator Loss: 0.4989\n",
      "Epoch [83/100], Step [201/938], Generator Loss: 3.4975, Discriminator Loss: 0.2130\n",
      "Epoch [83/100], Step [401/938], Generator Loss: 2.2813, Discriminator Loss: 0.2965\n",
      "Epoch [83/100], Step [601/938], Generator Loss: 1.8559, Discriminator Loss: 0.2691\n",
      "Epoch [83/100], Step [801/938], Generator Loss: 2.0683, Discriminator Loss: 0.2249\n",
      "Epoch [84/100], Step [1/938], Generator Loss: 1.3799, Discriminator Loss: 0.3770\n",
      "Epoch [84/100], Step [201/938], Generator Loss: 2.5277, Discriminator Loss: 0.2699\n",
      "Epoch [84/100], Step [401/938], Generator Loss: 2.4968, Discriminator Loss: 0.3508\n",
      "Epoch [84/100], Step [601/938], Generator Loss: 2.3683, Discriminator Loss: 0.2436\n",
      "Epoch [84/100], Step [801/938], Generator Loss: 2.2227, Discriminator Loss: 0.3586\n",
      "Epoch [85/100], Step [1/938], Generator Loss: 3.3018, Discriminator Loss: 0.1817\n",
      "Epoch [85/100], Step [201/938], Generator Loss: 2.3113, Discriminator Loss: 0.2455\n",
      "Epoch [85/100], Step [401/938], Generator Loss: 1.1746, Discriminator Loss: 0.3506\n",
      "Epoch [85/100], Step [601/938], Generator Loss: 2.9189, Discriminator Loss: 0.3036\n",
      "Epoch [85/100], Step [801/938], Generator Loss: 3.4648, Discriminator Loss: 0.3536\n",
      "Epoch [86/100], Step [1/938], Generator Loss: 1.2414, Discriminator Loss: 0.5246\n",
      "Epoch [86/100], Step [201/938], Generator Loss: 2.0352, Discriminator Loss: 0.2966\n",
      "Epoch [86/100], Step [401/938], Generator Loss: 1.8661, Discriminator Loss: 0.2816\n",
      "Epoch [86/100], Step [601/938], Generator Loss: 2.5292, Discriminator Loss: 0.2829\n",
      "Epoch [86/100], Step [801/938], Generator Loss: 1.8783, Discriminator Loss: 0.2668\n",
      "Epoch [87/100], Step [1/938], Generator Loss: 3.6834, Discriminator Loss: 0.2714\n",
      "Epoch [87/100], Step [201/938], Generator Loss: 2.8389, Discriminator Loss: 0.2938\n",
      "Epoch [87/100], Step [401/938], Generator Loss: 2.4150, Discriminator Loss: 0.3708\n",
      "Epoch [87/100], Step [601/938], Generator Loss: 1.9603, Discriminator Loss: 0.2748\n",
      "Epoch [87/100], Step [801/938], Generator Loss: 2.7022, Discriminator Loss: 0.1667\n",
      "Epoch [88/100], Step [1/938], Generator Loss: 4.7423, Discriminator Loss: 0.4827\n",
      "Epoch [88/100], Step [201/938], Generator Loss: 1.9982, Discriminator Loss: 0.2677\n",
      "Epoch [88/100], Step [401/938], Generator Loss: 2.2474, Discriminator Loss: 0.3428\n",
      "Epoch [88/100], Step [601/938], Generator Loss: 3.0606, Discriminator Loss: 0.3515\n",
      "Epoch [88/100], Step [801/938], Generator Loss: 2.1759, Discriminator Loss: 0.2454\n",
      "Epoch [89/100], Step [1/938], Generator Loss: 2.5140, Discriminator Loss: 0.2173\n",
      "Epoch [89/100], Step [201/938], Generator Loss: 1.6078, Discriminator Loss: 0.4630\n",
      "Epoch [89/100], Step [401/938], Generator Loss: 2.3721, Discriminator Loss: 0.3141\n",
      "Epoch [89/100], Step [601/938], Generator Loss: 2.7798, Discriminator Loss: 0.2152\n",
      "Epoch [89/100], Step [801/938], Generator Loss: 1.4404, Discriminator Loss: 0.3299\n",
      "Epoch [90/100], Step [1/938], Generator Loss: 3.0211, Discriminator Loss: 0.3199\n",
      "Epoch [90/100], Step [201/938], Generator Loss: 2.2313, Discriminator Loss: 0.2407\n",
      "Epoch [90/100], Step [401/938], Generator Loss: 3.0072, Discriminator Loss: 0.3201\n",
      "Epoch [90/100], Step [601/938], Generator Loss: 2.3727, Discriminator Loss: 0.2081\n",
      "Epoch [90/100], Step [801/938], Generator Loss: 1.3442, Discriminator Loss: 0.3726\n",
      "Epoch [91/100], Step [1/938], Generator Loss: 2.9317, Discriminator Loss: 0.1588\n",
      "Epoch [91/100], Step [201/938], Generator Loss: 1.5498, Discriminator Loss: 0.3653\n",
      "Epoch [91/100], Step [401/938], Generator Loss: 3.0720, Discriminator Loss: 0.4183\n",
      "Epoch [91/100], Step [601/938], Generator Loss: 2.9367, Discriminator Loss: 0.1824\n",
      "Epoch [91/100], Step [801/938], Generator Loss: 2.0982, Discriminator Loss: 0.1878\n",
      "Epoch [92/100], Step [1/938], Generator Loss: 1.1503, Discriminator Loss: 0.5932\n",
      "Epoch [92/100], Step [201/938], Generator Loss: 1.9611, Discriminator Loss: 0.2353\n",
      "Epoch [92/100], Step [401/938], Generator Loss: 2.4527, Discriminator Loss: 0.1812\n",
      "Epoch [92/100], Step [601/938], Generator Loss: 1.6790, Discriminator Loss: 0.3330\n",
      "Epoch [92/100], Step [801/938], Generator Loss: 3.3178, Discriminator Loss: 0.2779\n",
      "Epoch [93/100], Step [1/938], Generator Loss: 2.5127, Discriminator Loss: 0.2198\n",
      "Epoch [93/100], Step [201/938], Generator Loss: 2.9118, Discriminator Loss: 0.3091\n",
      "Epoch [93/100], Step [401/938], Generator Loss: 1.6054, Discriminator Loss: 0.5515\n",
      "Epoch [93/100], Step [601/938], Generator Loss: 2.1443, Discriminator Loss: 0.4063\n",
      "Epoch [93/100], Step [801/938], Generator Loss: 2.3643, Discriminator Loss: 0.3945\n",
      "Epoch [94/100], Step [1/938], Generator Loss: 2.7129, Discriminator Loss: 0.2517\n",
      "Epoch [94/100], Step [201/938], Generator Loss: 3.1088, Discriminator Loss: 0.2860\n",
      "Epoch [94/100], Step [401/938], Generator Loss: 4.1782, Discriminator Loss: 0.3454\n",
      "Epoch [94/100], Step [601/938], Generator Loss: 3.5424, Discriminator Loss: 0.2130\n",
      "Epoch [94/100], Step [801/938], Generator Loss: 2.7822, Discriminator Loss: 0.2185\n",
      "Epoch [95/100], Step [1/938], Generator Loss: 1.9162, Discriminator Loss: 0.3761\n",
      "Epoch [95/100], Step [201/938], Generator Loss: 2.5387, Discriminator Loss: 0.3291\n",
      "Epoch [95/100], Step [401/938], Generator Loss: 1.8952, Discriminator Loss: 0.2296\n",
      "Epoch [95/100], Step [601/938], Generator Loss: 2.8449, Discriminator Loss: 0.4560\n",
      "Epoch [95/100], Step [801/938], Generator Loss: 2.1057, Discriminator Loss: 0.2372\n",
      "Epoch [96/100], Step [1/938], Generator Loss: 3.4666, Discriminator Loss: 0.0650\n",
      "Epoch [96/100], Step [201/938], Generator Loss: 2.2357, Discriminator Loss: 0.4615\n",
      "Epoch [96/100], Step [401/938], Generator Loss: 1.3618, Discriminator Loss: 0.2984\n",
      "Epoch [96/100], Step [601/938], Generator Loss: 2.7675, Discriminator Loss: 0.2941\n",
      "Epoch [96/100], Step [801/938], Generator Loss: 3.0735, Discriminator Loss: 0.2201\n",
      "Epoch [97/100], Step [1/938], Generator Loss: 2.8075, Discriminator Loss: 0.2459\n",
      "Epoch [97/100], Step [201/938], Generator Loss: 1.4653, Discriminator Loss: 0.4283\n",
      "Epoch [97/100], Step [401/938], Generator Loss: 2.4650, Discriminator Loss: 0.2023\n",
      "Epoch [97/100], Step [601/938], Generator Loss: 2.1671, Discriminator Loss: 0.2318\n",
      "Epoch [97/100], Step [801/938], Generator Loss: 3.0955, Discriminator Loss: 0.2532\n",
      "Epoch [98/100], Step [1/938], Generator Loss: 2.9266, Discriminator Loss: 0.3663\n",
      "Epoch [98/100], Step [201/938], Generator Loss: 2.7549, Discriminator Loss: 0.3061\n",
      "Epoch [98/100], Step [401/938], Generator Loss: 1.8309, Discriminator Loss: 0.3082\n",
      "Epoch [98/100], Step [601/938], Generator Loss: 3.1006, Discriminator Loss: 0.2804\n",
      "Epoch [98/100], Step [801/938], Generator Loss: 5.3470, Discriminator Loss: 0.2773\n",
      "Epoch [99/100], Step [1/938], Generator Loss: 2.0035, Discriminator Loss: 0.3111\n",
      "Epoch [99/100], Step [201/938], Generator Loss: 1.8619, Discriminator Loss: 0.3638\n",
      "Epoch [99/100], Step [401/938], Generator Loss: 3.1114, Discriminator Loss: 0.2302\n",
      "Epoch [99/100], Step [601/938], Generator Loss: 2.3840, Discriminator Loss: 0.2845\n",
      "Epoch [99/100], Step [801/938], Generator Loss: 2.0558, Discriminator Loss: 0.3677\n",
      "Epoch [100/100], Step [1/938], Generator Loss: 2.9017, Discriminator Loss: 0.2732\n",
      "Epoch [100/100], Step [201/938], Generator Loss: 1.2727, Discriminator Loss: 0.3533\n",
      "Epoch [100/100], Step [401/938], Generator Loss: 2.5751, Discriminator Loss: 0.2414\n",
      "Epoch [100/100], Step [601/938], Generator Loss: 1.2235, Discriminator Loss: 0.4439\n",
      "Epoch [100/100], Step [801/938], Generator Loss: 2.8279, Discriminator Loss: 0.3407\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (28,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48020\\969004215.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Label: {labels[i].item()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software\\Anaconda\\envs\\py37\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m                 \u001b[1;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Don't modify *func*'s signature, as boilerplate.py needs it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software\\Anaconda\\envs\\py37\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1412\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software\\Anaconda\\envs\\py37\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5479\u001b[0m                               **kwargs)\n\u001b[0;32m   5480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5481\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5482\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\software\\Anaconda\\envs\\py37\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    714\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m    715\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[1;32m--> 716\u001b[1;33m                             .format(self._A.shape))\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (28,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_classes + int(torch.prod(torch.tensor(img_shape))), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, labels):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_emb(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "img_shape = (28, 28)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(latent_dim, num_classes, img_shape).to(device)\n",
    "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
    "\n",
    "# Define loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=\"./mnist_data\", train=True, transform=transform, download=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, labels) in enumerate(train_dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        # Move real images and labels to device\n",
    "        real_images = real_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Train generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake_images = generator(noise, labels)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        generator_loss = adversarial_loss(discriminator(fake_images, labels), real_labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        # Train discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real and fake images\n",
    "        real_loss = adversarial_loss(discriminator(real_images, labels), real_labels)\n",
    "        fake_loss = adversarial_loss(discriminator(fake_images.detach(), labels), fake_labels)\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        # Backpropagation\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if i % 200 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Step [{i+1}/{len(train_dataloader)}], \"\n",
    "                f\"Generator Loss: {generator_loss.item():.4f}, \"\n",
    "                f\"Discriminator Loss: {discriminator_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "# Generate and plot some samples\n",
    "num_samples = 10\n",
    "noise = torch.randn(num_samples, latent_dim).to(device)\n",
    "labels = torch.arange(num_classes).repeat(num_samples // num_classes).to(device)\n",
    "fake_images = generator(noise, labels).detach().cpu()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_classes, ncols=num_samples // num_classes, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(fake_images[i][0], cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Label: {labels[i].item()}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the weights\n",
    "weights_path = \"model_weights.pth\"\n",
    "\n",
    "# Save the generator and discriminator weights\n",
    "torch.save(generator.state_dict(), \"generator\" + weights_path)\n",
    "torch.save(discriminator.state_dict(), \"discriminator\" + weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the generator and discriminator models\n",
    "generator = Generator(latent_dim, num_classes, img_shape).to(device)\n",
    "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "generator.load_state_dict(torch.load(\"generator\" + weights_path))\n",
    "discriminator.load_state_dict(torch.load(\"discriminator\" + weights_path))\n",
    "\n",
    "# Generate and plot some samples\n",
    "num_samples = 10\n",
    "noise = torch.randn(num_samples, latent_dim).to(device)\n",
    "labels = torch.arange(num_classes).repeat(num_samples // num_classes).to(device)\n",
    "fake_images = generator(noise, labels).detach().cpu()\n",
    "\n",
    "fig, axs = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "for i in range(num_samples):\n",
    "    axs[i].imshow(fake_images[i].squeeze(), cmap='gray')\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
